{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "knowing-mechanism",
   "metadata": {},
   "source": [
    "```{figure} ../images/banner.png\n",
    "---\n",
    "align: center\n",
    "name: banner\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-scientist",
   "metadata": {},
   "source": [
    "# Chapter 3 : Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-copyright",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-arbitration",
   "metadata": {},
   "source": [
    "- various data sources & file formats\n",
    "- general methods to load the data into spark dataframe \n",
    "- general methods to save the data into spark dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-schema",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "\n",
    "- [1. Various data sources & file formats](#1)\n",
    "- [2. Loading & Saving data from various data sources](#2)\n",
    "    - [2a. from text file](#3)\n",
    "    - [2b. from CSV file](#4)\n",
    "    - [2c. from JSON file](#5)\n",
    "    - [2d. from Parquet file](#6)\n",
    "    - [2e. from ORC file](#7)\n",
    "    - [2f. from AVRO file](#8)\n",
    "    - [2g. from whole Binary file](#9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-exposure",
   "metadata": {},
   "source": [
    "## Chapter Outline (Visual)\n",
    "Click on any one of the image to go to that section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-madrid",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter3/datasources.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-interface",
   "metadata": {},
   "source": [
    "A | B\n",
    "- | - \n",
    "<kbd>[![alt](img/chapter3/text.png)](#100)</kbd> |<kbd> [![alt](img/chapter3/csv.png)](#101)</kbd>\n",
    "<kbd>[![alt](img/chapter3/json.png)](#102)</kbd> |<kbd> [![alt](img/chapter3/parquet.png)](#103)</kbd>\n",
    "<kbd>[![alt](img/chapter3/orc.png)](#104) </kbd>|<kbd> [![alt](img/chapter3/avro.png)](#105)</kbd>\n",
    "<kbd>[![alt](img/chapter3/binary.png)](#106)</kbd> |<kbd> [![alt](img/chapter3/text.png)](#107)</kbd>\n",
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-beach",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-athens",
   "metadata": {},
   "source": [
    "# border: 1px solid blue;\n",
    "![alt text](img/chapter3/test1.png \"Title\"){: width=550 height=100 style=\"float:right; padding:16px\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-blame",
   "metadata": {},
   "source": [
    "<kbd> ![alt text](img/chapter2/rdd_dataframe.png \"Title\")</kbd>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-swing",
   "metadata": {},
   "source": [
    "[![alt text](img/chapter2/rdd_dataframe.png \"Title\")](#245)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-month",
   "metadata": {},
   "source": [
    "[![alt text](img/chapter2/list_dataframe.png \"Title\")](#23)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-reverse",
   "metadata": {},
   "source": [
    "[![alt text](img/chapter2/pd_spark.png \"Title\")](#23)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-fashion",
   "metadata": {},
   "source": [
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-enterprise",
   "metadata": {},
   "source": [
    "## 1. What is spark dataframe?\n",
    "A DataFrame simply represents a table of data with rows and columns. A simple analogy would be a spreadsheet with named columns.\n",
    "\n",
    "Spark Data Frame is a distributed collection of data organized into named columns. It can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDD, Lists, Pandas data frame. \n",
    "\n",
    "\n",
    "```{figure} img/chapter2/spark_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "lonely-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+---+----------+---------------------------------------+\n",
      "|_1  |_2 |_3  |_4 |_5        |_6                                     |\n",
      "+----+---+----+---+----------+---------------------------------------+\n",
      "|John|180|true|1.7|1960-01-01|{“home”: 123456789, “office”:234567567}|\n",
      "+----+---+----+---+----------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#alist = [(John\t180\tTrue\t1.70\t1960-01-01\t{“home”: 123456789, “office”:234567567}\t[“blue”,”red”,”green”]\t]\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "spark.createDataFrame([(\"John\",180,True, 1.7, \"1960-01-01\", '{“home”: 123456789, “office”:234567567}'),]).show(1,False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "atmospheric-technical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- favorite_colors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- smoker: boolean (nullable = true)\n",
      " |-- test: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonStrings = ['{\"name\":\"Yin\",\"age\":45,\"smoker\": true,\"test\":34, \"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"},\"favorite_colors\": [\"blue\",\"green\"] }',]\n",
    "otherPeopleRDD = spark.sparkContext.parallelize(jsonStrings)\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.printSchema()  \n",
    "#123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "placed-figure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+\n",
      "|name|weight|smoker|height|birthdate |phone_nos                               |favorite_colors |address                     |\n",
      "+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+\n",
      "|john|180   |true  |1.7   |1960-01-01|[office -> 123456789, home -> 223456789]|[blue, red]     |[100, street1, city1, 12345]|\n",
      "|tony|180   |true  |1.8   |1990-01-01|[office -> 223456789, home -> 323456789]|[green, purple] |[200, street2, city2, 22345]|\n",
      "|mike|180   |true  |1.65  |1980-01-01|[office -> 323456789, home -> 423456789]|[yellow, orange]|[300, street3, city3, 32345]|\n",
      "+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as func\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"weight\", LongType()),\n",
    "    StructField(\"smoker\", BooleanType()),\n",
    "    StructField(\"height\", DoubleType()),\n",
    "    StructField(\"birthdate\", StringType()),\n",
    "    StructField(\"phone_nos\", MapType(StringType(),LongType(),True),True),  \n",
    "    StructField(\"favorite_colors\", ArrayType(StringType(),True),True),  \n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"houseno\", IntegerType(),True),\n",
    "        StructField(\"street\", StringType(),True),\n",
    "        StructField(\"city\", StringType(),True),\n",
    "        StructField(\"zipcode\", IntegerType(),True),\n",
    "    ])) \n",
    "    \n",
    "])\n",
    "\n",
    "df = spark.createDataFrame((\n",
    "    [[\"john\",180,True,1.7,'1960-01-01',{'office': 123456789, 'home': 223456789},[\"blue\",\"red\"],(100,'street1','city1',12345)],\n",
    "    [\"tony\",180,True,1.8,'1990-01-01',{'office': 223456789, 'home': 323456789},[\"green\",\"purple\"],(200,'street2','city2',22345)],\n",
    "    [\"mike\",180,True,1.65,'1980-01-01',{'office': 323456789, 'home': 423456789},[\"yellow\",\"orange\"],(300,'street3','city3',32345)]]\n",
    "),schema=schema)\n",
    "df.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "modified-capital",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      " |-- smoker: boolean (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- phone_nos: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- favorite_colors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- houseno: integer (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- zipcode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "assigned-cisco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------+----------------+------+----+----------------------+------+------+\n",
      "|address                     |birthdate |favorite_colors |height|name|phone_nos             |smoker|weight|\n",
      "+----------------------------+----------+----------------+------+----+----------------------+------+------+\n",
      "|[city1, 100, street1, 12345]|1960-01-01|[blue, red]     |1.7   |john|[223456789, 123456789]|true  |180   |\n",
      "|[city2, 200, street2, 22345]|1990-01-01|[green, purple] |1.8   |tony|[323456789, 223456789]|true  |180   |\n",
      "|[city3, 300, street3, 32345]|1980-01-01|[yellow, orange]|1.65  |mike|[423456789, 323456789]|true  |180   |\n",
      "+----------------------------+----------+----------------+------+----+----------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#JSON FILE\n",
    "#df.repartition(1).write.json(\"/Users/deepak/Documents/sparkbook/chapters/data/json\")\n",
    "spark.read.format(\"json\").load(\"/Users/deepak/Documents/sparkbook/chapters/data/json\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "floating-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df.select(func.concat(\"name\",\"weight\",\"smoker\",\"height\",\"birthdate\",func.to_json(\"phone_nos\"),func.to_json(\"favorite_colors\"),func.to_json(\"address\")).alias(\"text\")).repartition(1).write.format(\"text\").option(\"header\",\"true\").save(\"/Users/deepak/Documents/sparkbook/chapters/data/people/part-00000-29f563bb-12e4-42c3-bae3-3e30505a16cc-c000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "exempt-swaziland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                             |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|john180true1.71960-01-01{\"office\":123456789,\"home\":223456789}[\"blue\",\"red\"]{\"houseno\":100,\"street\":\"street1\",\"city\":\"city1\",\"zipcode\":12345}      |\n",
      "|tony180true1.81990-01-01{\"office\":223456789,\"home\":323456789}[\"green\",\"purple\"]{\"houseno\":200,\"street\":\"street2\",\"city\":\"city2\",\"zipcode\":22345}  |\n",
      "|mike180true1.651980-01-01{\"office\":323456789,\"home\":423456789}[\"yellow\",\"orange\"]{\"houseno\":300,\"street\":\"street3\",\"city\":\"city3\",\"zipcode\":32345}|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"text\").load(\"/Users/deepak/Documents/sparkbook/chapters/data/people/part-00000-29f563bb-12e4-42c3-bae3-3e30505a16cc-c000.txt\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "southeast-thread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+\n",
      "|name|weight|smoker|height|birthdate |phone_nos                            |favorite_colors    |address                                                          |\n",
      "+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+\n",
      "|john|180   |true  |1.7   |1960-01-01|{\"office\":123456789,\"home\":223456789}|[\"blue\",\"red\"]     |{\"houseno\":100,\"street\":\"street1\",\"city\":\"city1\",\"zipcode\":12345}|\n",
      "|tony|180   |true  |1.8   |1990-01-01|{\"office\":223456789,\"home\":323456789}|[\"green\",\"purple\"] |{\"houseno\":200,\"street\":\"street2\",\"city\":\"city2\",\"zipcode\":22345}|\n",
      "|mike|180   |true  |1.65  |1980-01-01|{\"office\":323456789,\"home\":423456789}|[\"yellow\",\"orange\"]|{\"houseno\":300,\"street\":\"street3\",\"city\":\"city3\",\"zipcode\":32345}|\n",
      "+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/Users/deepak/Documents/sparkbook/chapters/data/people.csv', header=True).show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "plastic-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/Users/deepak/Documents/sparkbook/chapters\n",
    "from pyspark.sql import functions as func\n",
    "#df.select(\"name\",\"weight\",\"smoker\",\"height\",\"birthdate\",func.to_json(\"phone_nos\").alias(\"phone_nos\"),func.to_json(\"favorite_colors\").alias(\"favorite_colors\"),func.to_json(\"address\").alias(\"address\")).repartition(1).write.csv('/Users/deepak/Documents/sparkbook/chapters/data/people.csv', header=True)#show(3,False)\n",
    "#df.write.csv('/Users/deepak/Documents/sparkbook/chapters/data/people1.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "front-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.repartition(1).write.parquet(\"/Users/deepak/Documents/sparkbook/chapters/data/parquetfile\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "sorted-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.read.parquet(\"/Users/deepak/Documents/sparkbook/chapters/data/parquetfile\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "helpful-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.repartition(1).write.orc(\"/Users/deepak/Documents/sparkbook/chapters/data/orcfile\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "honest-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.read.orc(\"/Users/deepak/Documents/sparkbook/chapters/data/orcfile\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "moving-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.repartition(1).write.json(\"/Users/deepak/Documents/sparkbook/chapters/data/jsonfile\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-memorial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "guided-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.repartition(1).write.format(\"avro\").save(\"/Users/deepak/Documents/sparkbook/chapters/data/avrofile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "powerful-radius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/Users/deepa...|2021-01-19 21:26:53| 63864|[89 50 4E 47 0D 0...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.width = 0\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "spark.read.format(\"binaryFile\").load(\"/Users/deepak/Documents/sparkbook/images/banner.png\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-latter",
   "metadata": {},
   "source": [
    "## hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-louisiana",
   "metadata": {},
   "source": [
    "<a id='100'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "immediate-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_location = 'hive-warehouse'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "protected-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "silent-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myDf.createOrReplaceTempView(\"mytempTable\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "rational-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"hive.metastore.schema.verification\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "elegant-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.saveAsTable('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "inside-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"create table people as select * from df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "starting-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext.sql(\"create table mytable as select * from mytempTable\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "signed-indiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(weight,LongType,true),StructField(smoker,BooleanType,true),StructField(height,DoubleType,true),StructField(birthdate,StringType,true),StructField(phone_nos,MapType(StringType,LongType,true),true),StructField(favorite_colors,ArrayType(StringType,true),true),StructField(address,StructType(List(StructField(houseno,IntegerType,true),StructField(street,StringType,true),StructField(city,StringType,true),StructField(zipcode,IntegerType,true))),true)))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"weight\", LongType()),\n",
    "    StructField(\"smoker\", BooleanType()),\n",
    "    StructField(\"height\", DoubleType()),\n",
    "    StructField(\"birthdate\", StringType()),\n",
    "    StructField(\"phone_nos\", MapType(StringType(),LongType(),True),True),  \n",
    "    StructField(\"favorite_colors\", ArrayType(StringType(),True),True),  \n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"houseno\", IntegerType(),True),\n",
    "        StructField(\"street\", StringType(),True),\n",
    "        StructField(\"city\", StringType(),True),\n",
    "        StructField(\"zipcode\", IntegerType(),True),\n",
    "    ])) \n",
    "    \n",
    "])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "piano-crack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello)\n"
     ]
    }
   ],
   "source": [
    "print(\"hello)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-sterling",
   "metadata": {},
   "source": [
    "<a id='1001'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-renaissance",
   "metadata": {},
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-literature",
   "metadata": {},
   "source": [
    "###  2. Creating a spark dataframe \n",
    "\n",
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)</b>\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "data – RDD,list, or pandas.DataFrame.\n",
    "\n",
    "schema – a pyspark.sql.types.DataType or a datatype string or a list of column names, default is None. \n",
    "\n",
    "samplingRatio – the sample ratio of rows used for inferring\n",
    "\n",
    "verifySchema – verify data types of every row against schema.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-institute",
   "metadata": {},
   "source": [
    "<a id='33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-amber",
   "metadata": {},
   "source": [
    "## 2a. from RDD\n",
    "\n",
    "\n",
    "```{figure} img/chapter2/rdd_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "<b>What is RDD?</b>\n",
    "\n",
    "Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. \n",
    "\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. \n",
    "\n",
    "RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. \n",
    "\n",
    "Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\n",
    "\n",
    "<b>Creating RDD :</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "prospective-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "rdd_spark = spark.sparkContext.parallelize([('John', 'Seattle', 60, True, 1.7, '1960-01-01'),\n",
    " ('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'),\n",
    " ('Mike', 'New York', 40, True, 1.65, '1980-01-01')]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "careful-immunology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'Seattle', 60, True, 1.7, '1960-01-01'), ('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), ('Mike', 'New York', 40, True, 1.65, '1980-01-01')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-shipping",
   "metadata": {},
   "source": [
    "<b>Creating a spark dataframe:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "activated-perfume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+-----+----+----------+\n",
      "|  _1|       _2| _3|   _4|  _5|        _6|\n",
      "+----+---------+---+-----+----+----------+\n",
      "|John|  Seattle| 60| true| 1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30|false| 1.8|1990-01-01|\n",
      "|Mike| New York| 40| true|1.65|1980-01-01|\n",
      "+----+---------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rdd_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-hopkins",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-jefferson",
   "metadata": {},
   "source": [
    "## 2b. from List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-election",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter2/list_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "pregnant-heading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+-----+----+----------+\n",
      "|  _1|       _2| _3|   _4|  _5|        _6|\n",
      "+----+---------+---+-----+----+----------+\n",
      "|John|  Seattle| 60| true| 1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30|false| 1.8|1990-01-01|\n",
      "|Mike| New York| 40| true|1.65|1980-01-01|\n",
      "+----+---------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-flour",
   "metadata": {},
   "source": [
    "<a id='23'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-eligibility",
   "metadata": {},
   "source": [
    "## 2c. from pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-longitude",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter2/pd_spark.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-value",
   "metadata": {},
   "source": [
    "<b>Input: pandas dataframe</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-presence",
   "metadata": {},
   "source": [
    "<b>Creating pandas dataframe</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "contained-editing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1960-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tony</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1990-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike</td>\n",
       "      <td>New York</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1980-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0          1   2      3     4           5\n",
       "0  John    Seattle  60   True  1.70  1960-01-01\n",
       "1  Tony  Cupertino  30  False  1.80  1990-01-01\n",
       "2  Mike   New York  40   True  1.65  1980-01-01"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.DataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')])\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-appearance",
   "metadata": {},
   "source": [
    "<b>Output: spark dataframe</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "green-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.createDataFrame(df_pd).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-patient",
   "metadata": {},
   "source": [
    "## .  &emsp; 3a. test1\n",
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-sitting",
   "metadata": {},
   "source": [
    "##  &emsp;  5. &emsp; &emsp; test2\n",
    "<a id='5'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
