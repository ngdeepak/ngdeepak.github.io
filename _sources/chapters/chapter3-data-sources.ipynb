{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "domestic-savings",
   "metadata": {},
   "source": [
    "```{figure} ../images/banner.png\n",
    "---\n",
    "align: center\n",
    "name: banner\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-arthritis",
   "metadata": {},
   "source": [
    "# Chapter 3 : Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-brake",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-horizontal",
   "metadata": {},
   "source": [
    "- various data sources & file formats\n",
    "- general methods to load the data into spark dataframe \n",
    "- general methods to save the data into spark dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-rating",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "\n",
    "- [1. Various data sources & file formats](#1)\n",
    "- [2. Loading & Saving data from various data sources](#2)\n",
    "    - [2a. from text file](#3)\n",
    "    - [2b. from CSV file](#4)\n",
    "    - [2c. from JSON file](#5)\n",
    "    - [2d. from Parquet file](#6)\n",
    "    - [2e. from ORC file](#7)\n",
    "    - [2f. from AVRO file](#8)\n",
    "    - [2g. from whole Binary file](#9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-vermont",
   "metadata": {},
   "source": [
    "## Chapter Outline (Visual)\n",
    "Click on any one of the image to go to that section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-mattress",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter3/datasources.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-imaging",
   "metadata": {},
   "source": [
    "## Chapter Outline (Visual)\n",
    "Click on any one of the image to go to that section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "auburn-contents",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      require([], function() {\n",
       "      })\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "\tif (!js_urls.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.3/dist/panel.min.js\"];\n",
       "  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/dataframe.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid red;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid red;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid red;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid blue;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid blue;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid red;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid black;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid black;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid black;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid black;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 1px solid black;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 1px solid black;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid black;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid blue;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid red;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid red;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid red;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid red;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid green;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      require([], function() {\n      })\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n\tif (!js_urls.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.3/dist/panel.min.js\"];\n  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/dataframe.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid red;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid red;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid red;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid blue;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid blue;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid red;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid black;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid black;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid black;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table * {\\n  border: 1px solid black;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 1px solid black;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 1px solid black;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid black;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid blue;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid red;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid red;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid red;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid red;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid green;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n    },\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import panel as pn\n",
    "css = \"\"\"\n",
    "div.special_table + table, th, td {\n",
    "  border: 3px solid orange;\n",
    "}\n",
    "\"\"\"\n",
    "pn.extension(raw_css=[css])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-choir",
   "metadata": {},
   "source": [
    "<div class=\"special_table\"></div>\n",
    "\n",
    "click on any of the image below |To come back to this image gallery, on the top right corner under contents, click on \"Outline Gallery\" \n",
    "- | - \n",
    "[![alt](img/chapter3/text.png)](#501)| [![alt](img/chapter3/csv.png)](#502)\n",
    "[![alt](img/chapter3/json.png)](#503) | [![alt](img/chapter3/parquet.png)](#504)\n",
    "[![alt](img/chapter3/orc.png)](#505) | [![alt](img/chapter3/avro.png)](#506)\n",
    "[![alt](img/chapter3/binary.png)](#507) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-milwaukee",
   "metadata": {},
   "source": [
    "# test123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-closure",
   "metadata": {},
   "source": [
    "[<img src=\"img/chapter3/text.png\" />](#12451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "animal-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "## [![alt text](img/chapter2/pd_spark.png \"Title\")](#23)\n",
    "#***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-newsletter",
   "metadata": {},
   "source": [
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-statistics",
   "metadata": {},
   "source": [
    "## 1. What is spark dataframe?\n",
    "A DataFrame simply represents a table of data with rows and columns. A simple analogy would be a spreadsheet with named columns.\n",
    "\n",
    "Spark Data Frame is a distributed collection of data organized into named columns. It can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDD, Lists, Pandas data frame. \n",
    "\n",
    "\n",
    "```{figure} img/chapter2/spark_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "green-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+---+----------+---------------------------------------+\n",
      "|_1  |_2 |_3  |_4 |_5        |_6                                     |\n",
      "+----+---+----+---+----------+---------------------------------------+\n",
      "|John|180|true|1.7|1960-01-01|{“home”: 123456789, “office”:234567567}|\n",
      "+----+---+----+---+----------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#alist = [(John\t180\tTrue\t1.70\t1960-01-01\t{“home”: 123456789, “office”:234567567}\t[“blue”,”red”,”green”]\t]\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "spark.createDataFrame([(\"John\",180,True, 1.7, \"1960-01-01\", '{“home”: 123456789, “office”:234567567}'),]).show(1,False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "oriental-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- favorite_colors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- smoker: boolean (nullable = true)\n",
      " |-- test: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonStrings = ['{\"name\":\"Yin\",\"age\":45,\"smoker\": true,\"test\":34, \"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"},\"favorite_colors\": [\"blue\",\"green\"] }',]\n",
    "otherPeopleRDD = spark.sparkContext.parallelize(jsonStrings)\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.printSchema()  \n",
    "#123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "hired-increase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+\n",
      "|name|weight|smoker|height|birthdate |phone_nos                               |favorite_colors |address                     |\n",
      "+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+\n",
      "|john|180   |true  |1.7   |1960-01-01|[office -> 123456789, home -> 223456789]|[blue, red]     |[100, street1, city1, 12345]|\n",
      "|tony|180   |true  |1.8   |1990-01-01|[office -> 223456789, home -> 323456789]|[green, purple] |[200, street2, city2, 22345]|\n",
      "|mike|180   |true  |1.65  |1980-01-01|[office -> 323456789, home -> 423456789]|[yellow, orange]|[300, street3, city3, 32345]|\n",
      "+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as func\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"weight\", LongType()),\n",
    "    StructField(\"smoker\", BooleanType()),\n",
    "    StructField(\"height\", DoubleType()),\n",
    "    StructField(\"birthdate\", StringType()),\n",
    "    StructField(\"phone_nos\", MapType(StringType(),LongType(),True),True),  \n",
    "    StructField(\"favorite_colors\", ArrayType(StringType(),True),True),  \n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"houseno\", IntegerType(),True),\n",
    "        StructField(\"street\", StringType(),True),\n",
    "        StructField(\"city\", StringType(),True),\n",
    "        StructField(\"zipcode\", IntegerType(),True),\n",
    "    ])) \n",
    "    \n",
    "])\n",
    "\n",
    "df = spark.createDataFrame((\n",
    "    [[\"john\",180,True,1.7,'1960-01-01',{'office': 123456789, 'home': 223456789},[\"blue\",\"red\"],(100,'street1','city1',12345)],\n",
    "    [\"tony\",180,True,1.8,'1990-01-01',{'office': 223456789, 'home': 323456789},[\"green\",\"purple\"],(200,'street2','city2',22345)],\n",
    "    [\"mike\",180,True,1.65,'1980-01-01',{'office': 323456789, 'home': 423456789},[\"yellow\",\"orange\"],(300,'street3','city3',32345)]]\n",
    "),schema=schema)\n",
    "df.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "funny-prison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      " |-- smoker: boolean (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- phone_nos: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- favorite_colors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- houseno: integer (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- zipcode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "varying-spectacular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------+----------------+------+----+----------------------+------+------+\n",
      "|address                     |birthdate |favorite_colors |height|name|phone_nos             |smoker|weight|\n",
      "+----------------------------+----------+----------------+------+----+----------------------+------+------+\n",
      "|[city1, 100, street1, 12345]|1960-01-01|[blue, red]     |1.7   |john|[223456789, 123456789]|true  |180   |\n",
      "|[city2, 200, street2, 22345]|1990-01-01|[green, purple] |1.8   |tony|[323456789, 223456789]|true  |180   |\n",
      "|[city3, 300, street3, 32345]|1980-01-01|[yellow, orange]|1.65  |mike|[423456789, 323456789]|true  |180   |\n",
      "+----------------------------+----------+----------------+------+----+----------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#JSON FILE\n",
    "#df.repartition(1).write.json(\"/Users/deepak/Documents/sparkbook/chapters/data/json\")\n",
    "spark.read.format(\"json\").load(\"/Users/deepak/Documents/sparkbook/chapters/data/json\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dying-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df.select(func.concat(\"name\",\"weight\",\"smoker\",\"height\",\"birthdate\",func.to_json(\"phone_nos\"),func.to_json(\"favorite_colors\"),func.to_json(\"address\")).alias(\"text\")).repartition(1).write.format(\"text\").option(\"header\",\"true\").save(\"/Users/deepak/Documents/sparkbook/chapters/data/people/part-00000-29f563bb-12e4-42c3-bae3-3e30505a16cc-c000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "weird-point",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                             |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|john180true1.71960-01-01{\"office\":123456789,\"home\":223456789}[\"blue\",\"red\"]{\"houseno\":100,\"street\":\"street1\",\"city\":\"city1\",\"zipcode\":12345}      |\n",
      "|tony180true1.81990-01-01{\"office\":223456789,\"home\":323456789}[\"green\",\"purple\"]{\"houseno\":200,\"street\":\"street2\",\"city\":\"city2\",\"zipcode\":22345}  |\n",
      "|mike180true1.651980-01-01{\"office\":323456789,\"home\":423456789}[\"yellow\",\"orange\"]{\"houseno\":300,\"street\":\"street3\",\"city\":\"city3\",\"zipcode\":32345}|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"text\").load(\"/Users/deepak/Documents/sparkbook/chapters/data/people/part-00000-29f563bb-12e4-42c3-bae3-3e30505a16cc-c000.txt\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "simple-charity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+\n",
      "|name|weight|smoker|height|birthdate |phone_nos                            |favorite_colors    |address                                                          |\n",
      "+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+\n",
      "|john|180   |true  |1.7   |1960-01-01|{\"office\":123456789,\"home\":223456789}|[\"blue\",\"red\"]     |{\"houseno\":100,\"street\":\"street1\",\"city\":\"city1\",\"zipcode\":12345}|\n",
      "|tony|180   |true  |1.8   |1990-01-01|{\"office\":223456789,\"home\":323456789}|[\"green\",\"purple\"] |{\"houseno\":200,\"street\":\"street2\",\"city\":\"city2\",\"zipcode\":22345}|\n",
      "|mike|180   |true  |1.65  |1980-01-01|{\"office\":323456789,\"home\":423456789}|[\"yellow\",\"orange\"]|{\"houseno\":300,\"street\":\"street3\",\"city\":\"city3\",\"zipcode\":32345}|\n",
      "+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/Users/deepak/Documents/sparkbook/chapters/data/people.csv', header=True).show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "younger-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/Users/deepak/Documents/sparkbook/chapters\n",
    "from pyspark.sql import functions as func\n",
    "#df.select(\"name\",\"weight\",\"smoker\",\"height\",\"birthdate\",func.to_json(\"phone_nos\").alias(\"phone_nos\"),func.to_json(\"favorite_colors\").alias(\"favorite_colors\"),func.to_json(\"address\").alias(\"address\")).repartition(1).write.csv('/Users/deepak/Documents/sparkbook/chapters/data/people.csv', header=True)#show(3,False)\n",
    "#df.write.csv('/Users/deepak/Documents/sparkbook/chapters/data/people1.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "incident-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.repartition(1).write.parquet(\"/Users/deepak/Documents/sparkbook/chapters/data/parquetfile\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "promotional-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.read.parquet(\"/Users/deepak/Documents/sparkbook/chapters/data/parquetfile\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "quick-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.repartition(1).write.orc(\"/Users/deepak/Documents/sparkbook/chapters/data/orcfile\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "great-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.read.orc(\"/Users/deepak/Documents/sparkbook/chapters/data/orcfile\").show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "exact-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.repartition(1).write.json(\"/Users/deepak/Documents/sparkbook/chapters/data/jsonfile\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "configured-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.repartition(1).write.format(\"avro\").save(\"/Users/deepak/Documents/sparkbook/chapters/data/avrofile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "raised-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/Users/deepa...|2021-01-19 21:26:53| 63864|[89 50 4E 47 0D 0...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.width = 0\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "spark.read.format(\"binaryFile\").load(\"/Users/deepak/Documents/sparkbook/images/banner.png\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-international",
   "metadata": {},
   "source": [
    "## hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-prize",
   "metadata": {},
   "source": [
    "<a id='501'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-techno",
   "metadata": {},
   "source": [
    "<a id='502'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-pollution",
   "metadata": {},
   "source": [
    "<a id='503'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-absorption",
   "metadata": {},
   "source": [
    "<a id='504'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-helping",
   "metadata": {},
   "source": [
    "<a id='505'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-dance",
   "metadata": {},
   "source": [
    "<a id='506'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-douglas",
   "metadata": {},
   "source": [
    "<a id='507'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fresh-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_location = 'hive-warehouse'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "narrow-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "recorded-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myDf.createOrReplaceTempView(\"mytempTable\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "jewish-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"hive.metastore.schema.verification\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "clinical-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.saveAsTable('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "unknown-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"create table people as select * from df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "located-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlContext.sql(\"create table mytable as select * from mytempTable\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "split-model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(weight,LongType,true),StructField(smoker,BooleanType,true),StructField(height,DoubleType,true),StructField(birthdate,StringType,true),StructField(phone_nos,MapType(StringType,LongType,true),true),StructField(favorite_colors,ArrayType(StringType,true),true),StructField(address,StructType(List(StructField(houseno,IntegerType,true),StructField(street,StringType,true),StructField(city,StringType,true),StructField(zipcode,IntegerType,true))),true)))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"weight\", LongType()),\n",
    "    StructField(\"smoker\", BooleanType()),\n",
    "    StructField(\"height\", DoubleType()),\n",
    "    StructField(\"birthdate\", StringType()),\n",
    "    StructField(\"phone_nos\", MapType(StringType(),LongType(),True),True),  \n",
    "    StructField(\"favorite_colors\", ArrayType(StringType(),True),True),  \n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"houseno\", IntegerType(),True),\n",
    "        StructField(\"street\", StringType(),True),\n",
    "        StructField(\"city\", StringType(),True),\n",
    "        StructField(\"zipcode\", IntegerType(),True),\n",
    "    ])) \n",
    "    \n",
    "])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "appointed-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello)\n"
     ]
    }
   ],
   "source": [
    "print(\"hello)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-douglas",
   "metadata": {},
   "source": [
    "<a id='1001'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-arrangement",
   "metadata": {},
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-facing",
   "metadata": {},
   "source": [
    "###  2. Creating a spark dataframe \n",
    "\n",
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)</b>\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "data – RDD,list, or pandas.DataFrame.\n",
    "\n",
    "schema – a pyspark.sql.types.DataType or a datatype string or a list of column names, default is None. \n",
    "\n",
    "samplingRatio – the sample ratio of rows used for inferring\n",
    "\n",
    "verifySchema – verify data types of every row against schema.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-conditioning",
   "metadata": {},
   "source": [
    "<a id='33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-perspective",
   "metadata": {},
   "source": [
    "## 2a. from RDD\n",
    "\n",
    "\n",
    "```{figure} img/chapter2/rdd_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "<b>What is RDD?</b>\n",
    "\n",
    "Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. \n",
    "\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. \n",
    "\n",
    "RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. \n",
    "\n",
    "Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\n",
    "\n",
    "<b>Creating RDD :</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "useful-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "rdd_spark = spark.sparkContext.parallelize([('John', 'Seattle', 60, True, 1.7, '1960-01-01'),\n",
    " ('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'),\n",
    " ('Mike', 'New York', 40, True, 1.65, '1980-01-01')]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "provincial-remove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'Seattle', 60, True, 1.7, '1960-01-01'), ('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), ('Mike', 'New York', 40, True, 1.65, '1980-01-01')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-crossing",
   "metadata": {},
   "source": [
    "<b>Creating a spark dataframe:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "desirable-blake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+-----+----+----------+\n",
      "|  _1|       _2| _3|   _4|  _5|        _6|\n",
      "+----+---------+---+-----+----+----------+\n",
      "|John|  Seattle| 60| true| 1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30|false| 1.8|1990-01-01|\n",
      "|Mike| New York| 40| true|1.65|1980-01-01|\n",
      "+----+---------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rdd_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-backing",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-block",
   "metadata": {},
   "source": [
    "## 2b. from List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-evolution",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter2/list_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "residential-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+-----+----+----------+\n",
      "|  _1|       _2| _3|   _4|  _5|        _6|\n",
      "+----+---------+---+-----+----+----------+\n",
      "|John|  Seattle| 60| true| 1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30|false| 1.8|1990-01-01|\n",
      "|Mike| New York| 40| true|1.65|1980-01-01|\n",
      "+----+---------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-lotus",
   "metadata": {},
   "source": [
    "<a id='12451'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-bicycle",
   "metadata": {},
   "source": [
    "## 2c. from pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-privacy",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter2/pd_spark.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-trademark",
   "metadata": {},
   "source": [
    "<b>Input: pandas dataframe</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-commerce",
   "metadata": {},
   "source": [
    "<b>Creating pandas dataframe</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "laughing-carnival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1960-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tony</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1990-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike</td>\n",
       "      <td>New York</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1980-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0          1   2      3     4           5\n",
       "0  John    Seattle  60   True  1.70  1960-01-01\n",
       "1  Tony  Cupertino  30  False  1.80  1990-01-01\n",
       "2  Mike   New York  40   True  1.65  1980-01-01"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.DataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')])\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-barrier",
   "metadata": {},
   "source": [
    "<b>Output: spark dataframe</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "sharp-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.createDataFrame(df_pd).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-national",
   "metadata": {},
   "source": [
    "## .  &emsp; 3a. test1\n",
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-brazil",
   "metadata": {},
   "source": [
    "##  &emsp;  5. &emsp; &emsp; test2\n",
    "<a id='5'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
