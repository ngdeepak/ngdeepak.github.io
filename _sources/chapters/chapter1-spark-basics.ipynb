{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "designing-label",
   "metadata": {},
   "source": [
    "```{figure} ../images/banner.png\n",
    "---\n",
    "align: center\n",
    "name: banner\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-reasoning",
   "metadata": {},
   "source": [
    "# Chapter 1: Spark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-biodiversity",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "\n",
    "- [1. What is Apache Spark?](#1)\n",
    "- [2. What is distributed data?](#2)\n",
    "- [3. Overview of Spark Cluster ](#3)\n",
    "- [4. How to create spark session? ](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-earth",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-blame",
   "metadata": {},
   "source": [
    "- Understand what Apache Spark is\n",
    "- Understand distributed data vs non-distributed data\n",
    "- How distributed data is stored in spark?\n",
    "- Overview of Spark Cluster\n",
    "- Understand the working of distributed computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-plastic",
   "metadata": {},
   "source": [
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-lightweight",
   "metadata": {},
   "source": [
    "## 1. What is Apache Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-relation",
   "metadata": {},
   "source": [
    "Apache Spark is a unified analytics engine for large-scale data processing.\n",
    "It consists of various modules as shown in the diagram below\n",
    "```{figure} img/chapter1/spark_platform.png\n",
    "---\n",
    "align: center\n",
    "name: \n",
    "---\n",
    "```\n",
    "Benefits:\n",
    "- <b>Speed</b> : Run workloads 100x faster.\n",
    "- <b>Ease of Use</b> : Write applications quickly in Java, Scala, Python, R, and SQL.\n",
    "- <b>Generality</b> : Combine SQL, streaming, and complex analytics.\n",
    "- <b>Runs Everywhere</b> : Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. It can access diverse data sources\n",
    "\n",
    "```{figure} img/chapter1/spark_runs.png\n",
    "---\n",
    "align: center\n",
    "name: \n",
    "---\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-hamilton",
   "metadata": {},
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-magnitude",
   "metadata": {},
   "source": [
    "## 2. What is distributed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-hardware",
   "metadata": {},
   "source": [
    "To answer this question, let us first understand how a non-distributed data look like. Lets us assume that a table consisting of just 3 records is stored on mySQL RDBMS in Linux.\n",
    "\n",
    "```{figure} img/chapter1/non-distributed-data.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "Having understood on how normal data is stored, let us look at how the same data is stored in a distributed data environment. Please note that distributed File System is required to store the data in distributed manner. \n",
    "\n",
    "Lets take a moment to understand what a distributed File System is.\n",
    "\n",
    "```{figure} img/chapter1/dist_file_system.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "Below is an example of how data is distributed across 3 nodes. In this example, table consisting of 2 columns(Name & Age) and 3 rows are stored across 3 worker nodes .  Each node stores one record.\n",
    "In spark, data operations are mostly carried out directly on a node where data resides instead of getting the data from other nodes. \n",
    "\n",
    "### Overview of how data is stored on a spark cluster\n",
    "\n",
    "```{figure} img/chapter1/spark_dist_data.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-flooring",
   "metadata": {},
   "source": [
    "<a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-conversion",
   "metadata": {},
   "source": [
    "## 3. Overview of Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-minneapolis",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter1/spark_cluster.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "Spark uses a master/slave architecture. As you can see in the figure above, it has one central coordinator (Driver) that communicates with many distributed workers (executors). The driver and each of the executors run in their own Java processes.\n",
    "\n",
    "<b>DRIVER</b>\n",
    "\n",
    "The driver is the process where the main method runs. First it converts the user program into tasks and after that it schedules the tasks on the executors.\n",
    "\n",
    "<b>EXECUTORS</b>\n",
    "\n",
    "Executors are worker nodes' processes in charge of running individual tasks in a given Spark job. They are launched at the beginning of a Spark application and typically run for the entire lifetime of an application. Once they have run the task they send the results to the driver. They also provide in-memory storage for RDDs that are cached by user programs through Block Manager.\n",
    "\n",
    "<b>APPLICATION EXECUTION FLOW</b>\n",
    "\n",
    "With this in mind, when you submit an application to the cluster with spark-submit this is what happens internally:\n",
    "\n",
    "1. A standalone application starts and instantiates a SparkContext instance (and it is only then when you can call the application a driver).\n",
    "2. The driver program ask for resources to the cluster manager to launch executors.\n",
    "3. The cluster manager launches executors.\n",
    "4. The driver process runs through the user application. Depending on the actions and transformations over RDDs task are sent to executors.\n",
    "5. Executors run the tasks and save the results.\n",
    "6. If any worker crashes, its tasks will be sent to different executors to be processed again. \n",
    "Spark automatically deals with failed or slow machines by re-executing failed or slow tasks. For example, if the node running a partition of a map() operation crashes, Spark will rerun it on another node; and even if the node does not crash but is simply much slower than other nodes, Spark can preemptively launch a “speculative” copy of the task on another node, and take its result if that finishes.\n",
    "7. With SparkContext.stop() from the driver or if the main method exits/crashes all the executors will be terminated and the cluster resources will be released by the cluster manager.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-monster",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-beaver",
   "metadata": {},
   "source": [
    "## 4. How to create spark session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dental-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-print",
   "metadata": {},
   "source": [
    "You should see the following output to make sure spark session is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "awful-porcelain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.167:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9099acadf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
