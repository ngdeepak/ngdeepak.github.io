{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "industrial-disclosure",
   "metadata": {},
   "source": [
    "```{figure} ../images/banner.png\n",
    "---\n",
    "align: center\n",
    "name: banner\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-baghdad",
   "metadata": {},
   "source": [
    "# Chapter 7 : Date Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-oasis",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-delaware",
   "metadata": {},
   "source": [
    "- Various data operations on columns containing date strings, date and timestamps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-edwards",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "\n",
    "- [1. How to deal with timezone/format conversions?](#1)\n",
    "    - [1a. How to convert string timestamp to date format?](#2)\n",
    "    - [1b. How to convert  a string timestamp to datetime format?](#3)\n",
    "    - [1c. How to convert a timestamp to unix timestamp?](#4)\n",
    "    - [1d. How to convert unix timestamp to a string timestamp?](#5)\n",
    "    - [1e. How to convert from a given time zone to UTC timestamps?](#6)\n",
    "    - [1f. How to convert timestamp in UTC timezone to a given time zone?](#7)\n",
    "    - [1g. How to convert the date format?](#8)\n",
    "- [2. How to add and subtract on date columns?](#9)\n",
    "    - [2a. How to find no of days between 2 dates?](#10)\n",
    "    - [2b. How to find no of months between 2 dates?](#11)\n",
    "    - [2c. How to add no of days to a date?](#12)\n",
    "    - [2d. How to subtract no of days from a date?](#13)\n",
    "    - [2e. How to add no of months to a date?](#14)\n",
    "- [3. How to extract year, month, date, hour, minute, second, day, weekday, week/quarter of year from a datetime column?](#15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authentic-toyota",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "from IPython.display import display_html\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html(index=False)\n",
    "        html_str+= \"\\xa0\\xa0\\xa0\"*10\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "space = \"\\xa0\" * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sitting-representation",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      require([], function() {\n",
       "      })\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "\tif (!js_urls.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.3/dist/panel.min.js\"];\n",
       "  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/dataframe.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      require([], function() {\n      })\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n\tif (!js_urls.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.3/dist/panel.min.js\"];\n  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/dataframe.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n    },\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n    },\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import panel as pn\n",
    "\n",
    "css = \"\"\"\n",
    "div.special_table + table, th, td {\n",
    "  border: 3px solid orange;\n",
    "}\n",
    "\"\"\"\n",
    "pn.extension(raw_css=[css])\n",
    "#<div class=\"special_table\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-bridal",
   "metadata": {},
   "source": [
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-topic",
   "metadata": {},
   "source": [
    "##  Chapter Outline - Gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-youth",
   "metadata": {},
   "source": [
    "### <b>Timezone format conversions</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-lemon",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "click on  | any image\n",
    "---: |:---   \n",
    "[![alt](img/chapter7/1.png)](#2)| [![alt](img/chapter7/2.png)](#3)\n",
    "[![alt](img/chapter7/3.png)](#4) | [![alt](img/chapter7/4.png)](#5)\n",
    "[![alt](img/chapter7/5.png)](#6)| [![alt](img/chapter7/6.png)](#7)\n",
    "[![alt](img/chapter7/7.png)](#8) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-blind",
   "metadata": {},
   "source": [
    "### <b> Addition and subtraction on date columns  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-silence",
   "metadata": {},
   "source": [
    "<div class=\"special_table\"></div>\n",
    "\n",
    "click on  | any image\n",
    "---: |:--- \n",
    "[![alt](img/chapter7/8.png)](#10)| [![alt](img/chapter7/9.png)](#11)\n",
    "[![alt](img/chapter7/10.png)](#12) | [![alt](img/chapter7/11.png)](#13)\n",
    "[![alt](img/chapter7/12.png)](#14)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-pressing",
   "metadata": {},
   "source": [
    "### <b> Extract year, month, date, hour, minute, second, day, weekday, week/quarter of year from a datetime column  </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-championship",
   "metadata": {},
   "source": [
    "[![alt](img/chapter7/13.png)](#15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-storm",
   "metadata": {},
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-tunisia",
   "metadata": {},
   "source": [
    "## 1a. How to convert string timestamp to date format?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter7/1.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-tenant",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.to_date(col, format=None)</b>\n",
    "\n",
    "Converts a Column into pyspark.sql.types.DateType using the optionally specified format. \n",
    "\n",
    "Specify formats according to datetime pattern. By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted. \n",
    "\n",
    "Equivalent to col.cast(\"date\").\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-actor",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a column having a string of timestamp</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "featured-endorsement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   string_timestamp|\n",
      "+-------------------+\n",
      "|2020-07-15 16:52:44|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_str = spark.createDataFrame([('2020-07-15 16:52:44',)], ['string_timestamp'])\n",
    "df_str.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-massage",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a date column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "expected-recording",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-07-15|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df_date = df_str.select(to_date(df_str.string_timestamp).alias('date'))\n",
    "df_date.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-angle",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "excess-scott",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>string_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15 16:52:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_str.toPandas(),df_date.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-satellite",
   "metadata": {},
   "source": [
    "<a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-affair",
   "metadata": {},
   "source": [
    "## 1b. How to convert a string timestamp to datetime format?\n",
    "\n",
    "\n",
    "```{figure} img/chapter7/2.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-capital",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.to_timestamp(col, format=None)</b>\n",
    "\n",
    "Converts a Column into pyspark.sql.types.TimestampType using the optionally specified format. \n",
    "\n",
    "Specify formats according to datetime pattern. By default, it follows casting rules to pyspark.sql.types.TimestampType if the format is omitted. \n",
    "\n",
    "Equivalent to col.cast(\"timestamp\").\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-surfing",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a column having a string of timestamp</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "matched-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   string_timestamp|\n",
      "+-------------------+\n",
      "|2020-07-15 16:52:44|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_string = spark.createDataFrame([('2020-07-15 16:52:44',)], ['string_timestamp'])\n",
    "df_string.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-paste",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a timestamp column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "directed-subcommittee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           datetime|\n",
      "+-------------------+\n",
      "|2020-07-15 16:52:44|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df_datetime = df_string.select(to_timestamp(df_string.string_timestamp).alias('datetime'))\n",
    "df_datetime.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-jones",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "organizational-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>string_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15 16:52:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15 16:52:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_string.toPandas(),df_datetime.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-syntax",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-screw",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-poetry",
   "metadata": {},
   "source": [
    "## 1c. How to convert unix timestamp to a string timestamp?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter7/3.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-factor",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')</b>\n",
    "\n",
    "Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-internet",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a number column representing unix time </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "headed-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| unix_time|\n",
      "+----------+\n",
      "|1594846364|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unix = spark.createDataFrame([(1594846364,)], ['unix_time'])\n",
    "df_unix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-winner",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a column with a sliced string</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "crude-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|timestamp_string   |\n",
      "+-------------------+\n",
      "|2020-07-15 16:52:44|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "df_time = df_unix.select(from_unixtime(df_unix.unix_time).alias(\"timestamp_string\"))\n",
    "df_time.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-newton",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solid-rugby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>unix_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1594846364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>timestamp_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15 16:52:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_unix.toPandas(),df_time.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-brunswick",
   "metadata": {},
   "source": [
    "<a id='5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-ceremony",
   "metadata": {},
   "source": [
    "## 1d. How to convert a timestamp to unix timestamp?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter7/4.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-mattress",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')</b>\n",
    "\n",
    "Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-malta",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame consisting of a timestamp column in UTC </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "macro-bracelet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|timestamp              |\n",
      "+-----------------------+\n",
      "|2021-03-06 10:16:37.728|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "df = spark.createDataFrame([(),],)\n",
    "df = df.select(current_timestamp().alias(\"timestamp\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-package",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame consisting of a unix timestamp column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "retired-transcription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|unixtimestamp|\n",
      "+-------------+\n",
      "|1615043798   |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "df_unix = df.select(unix_timestamp(df.timestamp).alias(\"unixtimestamp\"))\n",
    "df_unix.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-manual",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "turkish-announcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-03-06 10:16:38.354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>unixtimestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1615043798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_unix.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-cooler",
   "metadata": {},
   "source": [
    "<a id='6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-personal",
   "metadata": {},
   "source": [
    "## 1e. How to convert timestamp in UTC timezone to a given time zone?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter7/5.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-uganda",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.from_utc_timestamp(timestamp, tz)</b>\n",
    "\n",
    "This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and renders that timestamp as a timestamp in the given time zone.\n",
    "\n",
    "However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to the given timezone.\n",
    "\n",
    "This function may return confusing result if the input is a string with timezone, e.g. ‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp according to the timezone in the string, and finally display the result by converting the timestamp to string according to the session local timezone.\n",
    "\n",
    "Parameters\n",
    "- timestamp – the column that contains timestamps\n",
    "- tz – A string detailing the time zone ID that the input should be adjusted to. It should be in the format of  \n",
    "  either region-based zone IDs or zone offsets. Region IDs must have the form ‘area/city’, such as \n",
    "  ‘America/Los_Angeles’. Zone offsets must be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also \n",
    "  ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’. Other short names are not recommended to use because they can \n",
    "   be ambiguous.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-produce",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame consisting of a timestamp column in UTC time zone </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prime-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|timestamp_UTC          |\n",
      "+-----------------------+\n",
      "|2021-03-06 10:16:38.637|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please note that below timestamp is in UTC.\n",
    "df_utc = spark.createDataFrame([(),],)\n",
    "df_utc = df_utc.select(current_timestamp().alias(\"timestamp_UTC\"))\n",
    "df_utc.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-proceeding",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame consisting of a timestamp column in different timezone</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "passive-bearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|timestamp in local TZ  |\n",
      "+-----------------------+\n",
      "|2021-03-06 06:16:38.885|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp\n",
    "df_tz = df_utc.select(from_utc_timestamp(df_utc.timestamp_UTC,'UTC-4').alias(\"timestamp in local TZ\"))\n",
    "df_tz.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-bracelet",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "thick-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>timestamp_UTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-03-06 10:16:39.043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>timestamp in local TZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-03-06 06:16:39.180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_utc.toPandas(),df_tz.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-swedish",
   "metadata": {},
   "source": [
    "<a id='8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-canada",
   "metadata": {},
   "source": [
    "## 1f. How to convert from a given time zone to UTC timestamps?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter7/6.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-ebony",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "{admonition} Syntax\n",
    "<b>pyspark.sql.functions.to_utc_timestamp(timestamp, tz)</b>\n",
    "\n",
    "This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given timezone, and renders that timestamp as a timestamp in UTC.\n",
    "\n",
    "However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not timezone-agnostic. So in Spark this function just shift the timestamp value from the given timezone to UTC timezone.\n",
    "\n",
    "This function may return confusing result if the input is a string with timezone, e.g. ‘2018-03-13T06:18:23+00:00’. The reason is that, Spark firstly cast the string to timestamp according to the timezone in the string, and finally display the result by converting the timestamp to string according to the session local timezone.\n",
    "\n",
    "<b>Parameters</b>\n",
    "\n",
    "- timestamp – the column that contains timestamps\n",
    "- tz – A string detailing the time zone ID that the input should be adjusted to. It should be in the format of either region-based zone IDs or zone offsets. Region IDs must have the form ‘area/city’, such as ‘America/Los_Angeles’. Zone offsets must be in the format ‘(+|-)HH:mm’, for example ‘-08:00’ or ‘+01:00’. Also ‘UTC’ and ‘Z’ are supported as aliases of ‘+00:00’. Other short names are not recommended to use because they can be ambiguous.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-property",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame consisting of a timestamp column in UTC time zone </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "humanitarian-prayer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|timestamp              |\n",
      "+-----------------------+\n",
      "|2021-03-06 10:16:39.411|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please note that local time zone is EST.\n",
    "df_est = spark.createDataFrame([(),],)\n",
    "df_est = df.select(current_timestamp().alias(\"timestamp\"))\n",
    "df_est.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-ceiling",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame consisting of a timestamp column in different timezone</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "negative-clerk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|timestamp in UTC       |\n",
      "+-----------------------+\n",
      "|2021-03-06 14:16:39.783|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_utc_timestamp\n",
    "df_utc = df_est.select(to_utc_timestamp(df_est.timestamp,'GMT-4').alias(\"timestamp in UTC\"))\n",
    "df_utc.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-gibraltar",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "august-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-03-06 10:16:39.962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>timestamp in UTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-03-06 14:16:40.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_est.toPandas(),df_utc.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-impact",
   "metadata": {},
   "source": [
    "<a id='8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-radius",
   "metadata": {},
   "source": [
    "## 1g. How to convert the date format?\n",
    "\n",
    "\n",
    "```{figure} img/chapter7/7.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-customs",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.date_format(date, format)</b>\n",
    "\n",
    "Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.\n",
    "\n",
    "A pattern could be for instance dd.MM.yyyy and could return a string like ‘18.03.1993’. All pattern letters of datetime pattern. can be used.\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "- col : column\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-capability",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a string date column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fourth-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|date_format1|\n",
      "+------------+\n",
      "|  2020-07-15|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_format1 = spark.createDataFrame([('2020-07-15',)], ['date_format1'])\n",
    "df_format1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-burns",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a formatted date</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "square-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|date_format2|\n",
      "+------------+\n",
      "|  07/15/2020|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "df_format2 = df_format1.select(date_format('date_format1', 'MM/dd/yyy').alias('date_format2'))\n",
    "df_format2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-delhi",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "progressive-armenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input                      output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date_format1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date_format2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>07/15/2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"input                     \",            \"output\")\n",
    "display_side_by_side(df_format1.toPandas(),df_format2.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-singapore",
   "metadata": {},
   "source": [
    "<a id='9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-evans",
   "metadata": {},
   "source": [
    "<a id='10'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-gardening",
   "metadata": {},
   "source": [
    "## 2a. How to find no of days between 2 dates?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter7/8.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-protein",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.datediff(end, start)</b>\n",
    "\n",
    "Returns the number of days from start to end.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-classification",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame consisting of  date columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "reasonable-september",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     date1|     date2|\n",
      "+----------+----------+\n",
      "|2020-07-15|2020-06-10|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dates = spark.createDataFrame([('2020-07-15','2020-06-10')], ['date1', 'date2'])\n",
    "df_dates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-container",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a days diff </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "strange-milwaukee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|diff|\n",
      "+----+\n",
      "|  35|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df_diff = df_dates.select(datediff(df_dates.date1,df_dates.date2).alias('diff'))\n",
    "df_diff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-patio",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "exotic-tobacco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date1</th>\n",
       "      <th>date2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>2020-06-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_dates.toPandas(),df_diff.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-algebra",
   "metadata": {},
   "source": [
    "<a id='11'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-password",
   "metadata": {},
   "source": [
    "## 2b. How to find no of months between 2 dates?\n",
    "\n",
    "\n",
    "```{figure} img/chapter7/9.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-newsletter",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.months_between(date1, date2, roundOff=True)</b>\n",
    "\n",
    "Returns number of months between dates date1 and date2. If date1 is later than date2, then the result is positive. If date1 and date2 are on the same day of month, or both are the last day of month, returns an integer (time of day will be ignored). The result is rounded off to 8 digits unless roundOff is set to False.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-typing",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with date columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "neither-chassis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|              date1|     date2|\n",
      "+-------------------+----------+\n",
      "|2020-07-15 11:32:00|2020-01-30|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_month = spark.createDataFrame([('2020-07-15 11:32:00', '2020-01-30')], ['date1', 'date2'])\n",
    "df_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-procedure",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with months between 2 dates</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unsigned-creek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|months_between(date1, date2, true)|\n",
      "+----------------------------------+\n",
      "|                        5.53163082|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between\n",
    "df_month_diff = df_month.select(months_between(df_month.date1, df_month.date2))\n",
    "df_month_diff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-capital",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "blond-plate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date1</th>\n",
       "      <th>date2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15 11:32:00</td>\n",
       "      <td>2020-01-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>months_between(date1, date2, true)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5.531631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_month.toPandas(),df_month_diff.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-closure",
   "metadata": {},
   "source": [
    "<a id='12'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-premiere",
   "metadata": {},
   "source": [
    "## 2c. How to add no of days to a date?\n",
    "\n",
    "\n",
    "```{figure} img/chapter7/10.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-michigan",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.date_add(start, days)</b>\n",
    "\n",
    "Returns the date that is days days after start\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-inclusion",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a string date column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "modular-career",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-07-15|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_today = spark.createDataFrame([('2020-07-15',)], ['date'])\n",
    "df_today.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-shopping",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a new date</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acting-worth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  next_day|\n",
      "+----------+\n",
      "|2020-07-16|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "df_tomo = df_today.select(date_add(df_today.date, 1).alias('next_day'))\n",
    "df_tomo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-seating",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unlike-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input                      output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>next_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"input                     \",            \"output\")\n",
    "display_side_by_side(df_today.toPandas(),df_tomo.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-pricing",
   "metadata": {},
   "source": [
    "<a id='13'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-commonwealth",
   "metadata": {},
   "source": [
    "## 2d. How to subtract no of days from a date?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter7/11.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-priest",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.date_sub(start, days)</b>\n",
    "\n",
    "Returns the date that is days days before start\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-point",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a date column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "expensive-ceremony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-07-15|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date = spark.createDataFrame([('2020-07-15',)], ['date'])\n",
    "df_date.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-compilation",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a date column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "resident-trademark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| yesterday|\n",
      "+----------+\n",
      "|2020-07-14|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_sub\n",
    "df_sub = df_date.select(date_sub(df_date.date, 1).alias('yesterday'))\n",
    "df_sub.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-graham",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "strange-magnet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_date.toPandas(),df_sub.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-pharmaceutical",
   "metadata": {},
   "source": [
    "<a id='14'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-stamp",
   "metadata": {},
   "source": [
    "## 2e. How to add no of months to a date?\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/chapter7/12.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-stock",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.add_months(start, months)[source]</b>\n",
    "\n",
    "Returns the date that is months months after start\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-activation",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame consisting of a date column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dangerous-buddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-07-15|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2020-07-15',)], ['date'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-skating",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame consisting of a date column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "instant-causing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|next_month|\n",
      "+----------+\n",
      "|2020-08-15|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import add_months\n",
    "df1 = df.select(add_months(df.date, 1).alias('next_month'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-nerve",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "alpine-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>next_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-08-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df1.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-choice",
   "metadata": {},
   "source": [
    "<a id='15'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-requirement",
   "metadata": {},
   "source": [
    "## How to extract year, month, date, hour, minute, second, day, weekday, week/quarter of year from a datetime column?\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/chapter7/13.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-closer",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame consisting of a datetime column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "special-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|timestamp              |\n",
      "+-----------------------+\n",
      "|2021-03-06 10:16:44.645|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(),],)\n",
    "df = df.select(current_timestamp().alias(\"timestamp\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-happening",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame consisting of many columns </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "spiritual-report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+-----+----------+---------+---------+----+------+------+----------+-------+\n",
      "|timestamp              |year|month|dayofmonth|dayofweek|dayofyear|hour|minute|second|weekofyear|quarter|\n",
      "+-----------------------+----+-----+----------+---------+---------+----+------+------+----------+-------+\n",
      "|2021-03-06 10:16:44.959|2021|3    |6         |7        |65       |10  |16    |44    |9         |1      |\n",
      "+-----------------------+----+-----+----------+---------+---------+----+------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, dayofweek, dayofyear, hour, minute, second, weekofyear, quarter\n",
    "df2 = df.select(df.timestamp,year(df.timestamp).alias(\"year\"), \n",
    "month(df.timestamp).alias(\"month\"), dayofmonth(df.timestamp).alias(\"dayofmonth\"), \n",
    "dayofweek(df.timestamp).alias(\"dayofweek\"), dayofyear(df.timestamp).alias(\"dayofyear\"), \n",
    "hour(df.timestamp).alias(\"hour\"), minute(df.timestamp).alias(\"minute\"), \n",
    "second(df.timestamp).alias(\"second\"), weekofyear(df.timestamp).alias(\"weekofyear\"), \n",
    "quarter(df.timestamp).alias(\"quarter\"))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-portrait",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "difficult-contemporary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                             Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-03-06 10:16:45.152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2021-03-06 10:16:45.247</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                            \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df2.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-gateway",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
