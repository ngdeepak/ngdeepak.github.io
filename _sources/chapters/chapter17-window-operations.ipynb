{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjusted-pharmacy",
   "metadata": {},
   "source": [
    "```{figure} ../images/banner.png\n",
    "---\n",
    "align: center\n",
    "name: banner\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-expense",
   "metadata": {},
   "source": [
    "# Chapter 17 : Window operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-criticism",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-length",
   "metadata": {},
   "source": [
    "- Various data operations on columns containing map. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-cookie",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "\n",
    "- [1. How to deal with map column?](#1)\n",
    "    - [1a. How to create a column of map type?](#2)\n",
    "    - [1b. How to read individual elements of a map column ?](#3)\n",
    "    - [1c. How to extract the keys from a map column?](#4)\n",
    "    - [1d. How to extract the values from a map column?](#5)\n",
    "    - [1e. How to convert a map column into an array column?](#6)\n",
    "    - [1f. How to create a map column from multiple array columns?](#7)\n",
    "    - [1g. How to combine multiple map columns into one?](#8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "sweet-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "from IPython.display import display_html\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html(index=False)\n",
    "        html_str+= \"\\xa0\\xa0\\xa0\"*10\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "space = \"\\xa0\" * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legislative-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      require([], function() {\n",
       "      })\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "\tif (!js_urls.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.3/dist/panel.min.js\"];\n",
       "  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/dataframe.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      require([], function() {\n      })\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n\tif (!js_urls.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.3/dist/panel.min.js\"];\n  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/dataframe.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n    },\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import panel as pn\n",
    "\n",
    "css = \"\"\"\n",
    "div.special_table + table, th, td {\n",
    "  border: 3px solid orange;\n",
    "}\n",
    "\"\"\"\n",
    "pn.extension(raw_css=[css])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-murray",
   "metadata": {},
   "source": [
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-yorkshire",
   "metadata": {},
   "source": [
    "##  Chapter Outline - Gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-petroleum",
   "metadata": {},
   "source": [
    "<div class=\"special_table\"></div>\n",
    "\n",
    "Click on any of the image below                 |To come back to this image gallery, on the top right corner under contents, click on Chapter Outline - Gallery\" \n",
    "- | - \n",
    "[![alt](img/chapter9/1.png)](#2)| [![alt](img/chapter9/2.png)](#3)\n",
    "[![alt](img/chapter9/3.png)](#4)| [![alt](img/chapter9/4.png)](#5)\n",
    "[![alt](img/chapter9/5.png)](#6)| [![alt](img/chapter9/6.png)](#7)\n",
    "[![alt](img/chapter9/7.png)](#8)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-circuit",
   "metadata": {},
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-tattoo",
   "metadata": {},
   "source": [
    "## 1a. How to create a column of map type?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter9/1a.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-chain",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.map_keys(col)</b>\n",
    "\n",
    "Returns an unordered array containing the keys of the map.\n",
    "\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "- col – name of column or expression\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-bibliography",
   "metadata": {},
   "source": [
    "At the top level there are mainly 3 types of joins:\n",
    "\n",
    "INNER\n",
    "OUTER\n",
    "CROSS\n",
    "\n",
    "INNER JOIN - fetches data if present in both the tables.\n",
    "\n",
    "OUTER JOIN are of 3 types:\n",
    "\n",
    "LEFT OUTER JOIN - fetches data if present in the left table.\n",
    "RIGHT OUTER JOIN - fetches data if present in the right table.\n",
    "FULL OUTER JOIN - fetches data if present in either of the two tables.\n",
    "\n",
    "CROSS JOIN, as the name suggests, does [n X m] that joins everything to everything.\n",
    "Similar to scenario where we simply lists the tables for joining (in the FROM clause of the SELECT statement), using commas to separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "french-allowance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary\n",
      "  sales      10    6000\n",
      "     hr       7    3000\n",
      "     it       5    5000\n",
      "  sales       2    6000\n",
      "     hr       3    2000\n",
      "     hr       4    6000\n",
      "     it       8    8000\n",
      "  sales       9    5000\n",
      "  sales       1    7000\n",
      "     it       6    6000\n"
     ]
    }
   ],
   "source": [
    "###### from pyspark.sql.functions import to_date\n",
    "df = spark.createDataFrame([\n",
    "    (\"sales\",10,6000),(\"hr\",7,3000),(\"it\",5,5000),(\"sales\",2,6000),\n",
    "    (\"hr\",3,2000),(\"hr\",4,6000),(\"it\",8,8000),(\"sales\",9,5000),\n",
    "    (\"sales\",1,7000),(\"it\",6,6000)],\n",
    "    [\"dept_id\",\"emp_id\",\"salary\"])\n",
    "d1 = df.toPandas()\n",
    "print(d1.to_string(index=False))#show(truncate=False)print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-entrepreneur",
   "metadata": {},
   "source": [
    "What are Window Functions?\n",
    "\n",
    "A window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. But unlike regular aggregate functions, use of a window function does not cause rows to become grouped into a single output row — the rows retain their separate identities. Behind the scenes, the window function is able to access more than just the current row of the query result.\n",
    "\n",
    "In the DataFrame API, we provide utility functions to define a window specification. Taking Python as an example, users can specify partitioning expressions and ordering expressions as follows.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "windowSpec = \\\n",
    "  Window \\\n",
    "    .partitionBy(...) \\\n",
    "    .orderBy(...)\n",
    "    \n",
    "In addition to the ordering and partitioning, users need to define the start boundary of the frame, the end boundary of the frame, and the type of the frame, which are three components of a frame specification.\n",
    "\n",
    "There are five types of boundaries, which are unboundedPreceding, unboundedFollowing, currentRow, <value> Preceding, and <value> Following. \n",
    "    \n",
    "unboundedPreceding and unboundedFollowing represent the first row of the partition and the last row of the partition, respectively. \n",
    "    \n",
    "For the other three types of boundaries, they specify the offset from the position of the current input row and their specific meanings are defined based on the type of the frame. \n",
    "    \n",
    "There are two types of frames, ROW frame and RANGE frame.\n",
    "    \n",
    "ROW frame\n",
    "\n",
    "ROW frames are based on physical offsets from the position of the current input row, which means thatcurrentRow, <value> Preceding, and <value> Following specifies a physical offset. \n",
    "    \n",
    "If currentRow is used as a boundary, it represents the current input row. \n",
    "<value> Preceding and <value> Following describes the number of rows appear before and after the current input row, respectively. \n",
    "    \n",
    "The following figure illustrates a ROW frame with a 1 Preceding as the start boundary and 1 FOLLOWING as the end boundary (ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING in the SQL syntax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "crucial-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary   sum\n",
      "  sales       1    7000  7000\n",
      "  sales       2    6000 13000\n",
      "  sales       9    5000 18000\n",
      "  sales      10    6000 17000\n",
      "     hr       3    2000  2000\n",
      "     hr       4    6000  8000\n",
      "     hr       7    3000 11000\n",
      "     it       5    5000  5000\n",
      "     it       6    6000 11000\n",
      "     it       8    8000 19000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import Window\n",
    "window = Window.partitionBy(\"dept_id\").orderBy(\"emp_id\").rowsBetween(-2,  0)\n",
    "print(df.withColumn(\"sum\", func.sum(\"salary\").over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "elegant-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary   sum\n",
      "  sales       1    7000 18000\n",
      "  sales       2    6000 17000\n",
      "  sales       9    5000 11000\n",
      "  sales      10    6000  6000\n",
      "     hr       3    2000 11000\n",
      "     hr       4    6000  9000\n",
      "     hr       7    3000  3000\n",
      "     it       5    5000 19000\n",
      "     it       6    6000 14000\n",
      "     it       8    8000  8000\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(\"emp_id\").rowsBetween(0, 2)\n",
    "print(df.withColumn(\"sum\", func.sum(\"salary\").over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "gothic-cooling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary   sum\n",
      "  sales      10    6000  6000\n",
      "     hr       7    3000  9000\n",
      "     it       5    5000 14000\n",
      "  sales       2    6000 14000\n",
      "     hr       3    2000 13000\n",
      "     hr       4    6000 14000\n",
      "     it       8    8000 16000\n",
      "  sales       9    5000 19000\n",
      "  sales       1    7000 20000\n",
      "     it       6    6000 18000\n"
     ]
    }
   ],
   "source": [
    "window = Window.rowsBetween(-2,  0)\n",
    "print(df.withColumn(\"sum\", func.sum(\"salary\").over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "exotic-airplane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+\n",
      "|dept_id|emp_id|salary|  sum|\n",
      "+-------+------+------+-----+\n",
      "|  sales|    10|  6000| 9000|\n",
      "|     hr|     7|  3000| 8000|\n",
      "|     it|     5|  5000|11000|\n",
      "|  sales|     2|  6000| 8000|\n",
      "|     hr|     3|  2000| 8000|\n",
      "|     hr|     4|  6000|14000|\n",
      "|     it|     8|  8000|13000|\n",
      "|  sales|     9|  5000|12000|\n",
      "|  sales|     1|  7000|13000|\n",
      "|     it|     6|  6000| 6000|\n",
      "+-------+------+------+-----+\n",
      "\n",
      "dept_id  emp_id  salary   sum\n",
      "  sales      10    6000  9000\n",
      "     hr       7    3000  8000\n",
      "     it       5    5000 11000\n",
      "  sales       2    6000  8000\n",
      "     hr       3    2000  8000\n",
      "     hr       4    6000 14000\n",
      "     it       8    8000 13000\n",
      "  sales       9    5000 12000\n",
      "  sales       1    7000 13000\n",
      "     it       6    6000  6000\n"
     ]
    }
   ],
   "source": [
    "window = Window.rowsBetween(0,  1)\n",
    "df.withColumn(\"sum\", func.sum(\"salary\").over(window)).show()\n",
    "print(df.withColumn(\"sum\", func.sum(\"salary\").over(window)).toPandas().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "proprietary-senator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary  rank\n",
      "  sales       1    7000     1\n",
      "  sales      10    6000     2\n",
      "  sales       2    6000     2\n",
      "  sales       9    5000     3\n",
      "     hr       4    6000     1\n",
      "     hr       7    3000     2\n",
      "     hr       3    2000     3\n",
      "     it       8    8000     1\n",
      "     it       6    6000     2\n",
      "     it       5    5000     3\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\").desc())\n",
    "print(df.withColumn(\"rank\", func.dense_rank().over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "racial-deputy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary  rank\n",
      "  sales       1    7000     1\n",
      "  sales      10    6000     2\n",
      "  sales       2    6000     2\n",
      "  sales       9    5000     4\n",
      "     hr       4    6000     1\n",
      "     hr       7    3000     2\n",
      "     hr       3    2000     3\n",
      "     it       8    8000     1\n",
      "     it       6    6000     2\n",
      "     it       5    5000     3\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\").desc())\n",
    "print(df.withColumn(\"rank\", func.rank().over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "educated-advocate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary  salary_bucket\n",
      "  sales       1    7000              1\n",
      "  sales      10    6000              2\n",
      "  sales       2    6000              3\n",
      "  sales       9    5000              4\n",
      "     hr       4    6000              1\n",
      "     hr       7    3000              2\n",
      "     hr       3    2000              3\n",
      "     it       8    8000              1\n",
      "     it       6    6000              2\n",
      "     it       5    5000              3\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\").desc())\n",
    "print(df.withColumn(\"salary_bucket\", func.ntile(4).over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "useful-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary  previousrow_salary\n",
      "  sales       1    7000                 NaN\n",
      "  sales      10    6000              7000.0\n",
      "  sales       2    6000              6000.0\n",
      "  sales       9    5000              6000.0\n",
      "     hr       4    6000                 NaN\n",
      "     hr       7    3000              6000.0\n",
      "     hr       3    2000              3000.0\n",
      "     it       8    8000                 NaN\n",
      "     it       6    6000              8000.0\n",
      "     it       5    5000              6000.0\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\").desc())\n",
    "print(df.withColumn(\"previousrow_salary\", func.lag('salary',1).over(window)).toPandas().to_string(index=False))#.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "chemical-census",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary  nextrow_salary\n",
      "  sales       1    7000          6000.0\n",
      "  sales      10    6000          6000.0\n",
      "  sales       2    6000          5000.0\n",
      "  sales       9    5000             NaN\n",
      "     hr       4    6000          3000.0\n",
      "     hr       7    3000          2000.0\n",
      "     hr       3    2000             NaN\n",
      "     it       8    8000          6000.0\n",
      "     it       6    6000          5000.0\n",
      "     it       5    5000             NaN\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\").desc())\n",
    "print(df.withColumn(\"nextrow_salary\", func.lead('salary',1).over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "collaborative-palestinian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary  percentile\n",
      "  sales       9    5000    0.000000\n",
      "  sales      10    6000    0.333333\n",
      "  sales       2    6000    0.333333\n",
      "  sales       1    7000    1.000000\n",
      "     hr       3    2000    0.000000\n",
      "     hr       7    3000    0.500000\n",
      "     hr       4    6000    1.000000\n",
      "     it       5    5000    0.000000\n",
      "     it       6    6000    0.500000\n",
      "     it       8    8000    1.000000\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\"))\n",
    "print(df.withColumn(\"percentile\", func.percent_rank().over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "therapeutic-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary  row_no\n",
      "  sales       9    5000       1\n",
      "  sales      10    6000       2\n",
      "  sales       2    6000       3\n",
      "  sales       1    7000       4\n",
      "     hr       3    2000       1\n",
      "     hr       7    3000       2\n",
      "     hr       4    6000       3\n",
      "     it       5    5000       1\n",
      "     it       6    6000       2\n",
      "     it       8    8000       3\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\"))\n",
    "print(df.withColumn(\"row_no\", func.row_number().over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "equipped-picking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept_id  emp_id  salary  cume_dist\n",
      "  sales       9    5000   0.250000\n",
      "  sales      10    6000   0.750000\n",
      "  sales       2    6000   0.750000\n",
      "  sales       1    7000   1.000000\n",
      "     hr       3    2000   0.333333\n",
      "     hr       7    3000   0.666667\n",
      "     hr       4    6000   1.000000\n",
      "     it       5    5000   0.333333\n",
      "     it       6    6000   0.666667\n",
      "     it       8    8000   1.000000\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy(\"dept_id\").orderBy(func.col(\"salary\"))\n",
    "print(df.withColumn(\"cume_dist\", func.cume_dist().over(window)).toPandas().to_string(index=False))#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-passage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-rouge",
   "metadata": {},
   "source": [
    "Left Join\n",
    "\n",
    "A left join returns all values from the left relation and the matched values from the right relation, or appends NULL if there is no match. It is also referred to as a left outer join.\n",
    "LEFT JOIN and LEFT OUTER JOIN are equivalent.\n",
    "\n",
    "LEFT  JOIN - fetches data if present in the left table and only matching records from the right table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "industrial-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1005</td>\n",
       "      <td>200</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>100</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1004</td>\n",
       "      <td>200</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1003</td>\n",
       "      <td>300</td>\n",
       "      <td>tony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1002</td>\n",
       "      <td>200</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  order_id  amount  name\n",
       "0            6      1005     200  None\n",
       "1            1      1001     100  john\n",
       "2            1      1004     200  john\n",
       "3            3      1003     300  tony\n",
       "4            2      1002     200  mike"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"left\").toPandas()#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ranging-judgment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+\n",
      "|customer_id|order_id|amount|name|\n",
      "+-----------+--------+------+----+\n",
      "|          6|    1005|   200|null|\n",
      "|          1|    1001|   100|john|\n",
      "|          1|    1004|   200|john|\n",
      "|          3|    1003|   300|tony|\n",
      "|          2|    1002|   200|mike|\n",
      "+-----------+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "negative-diabetes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+\n",
      "|customer_id|order_id|amount|name|\n",
      "+-----------+--------+------+----+\n",
      "|          6|    1005|   200|null|\n",
      "|          1|    1001|   100|john|\n",
      "|          1|    1004|   200|john|\n",
      "|          3|    1003|   300|tony|\n",
      "|          2|    1002|   200|mike|\n",
      "+-----------+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"leftouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-natural",
   "metadata": {},
   "source": [
    "Right Join\n",
    "\n",
    "RIGHT JOIN - fetches data if present in the right table even if there is no matching records in the right table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adolescent-receptor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>tony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  order_id  amount  name\n",
       "0            1    1001.0   100.0  john\n",
       "1            1    1004.0   200.0  john\n",
       "2            3    1003.0   300.0  tony\n",
       "3            2    1002.0   200.0  mike\n",
       "4            4       NaN     NaN  kent"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"right\").toPandas()#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "popular-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+\n",
      "|customer_id|order_id|amount|name|\n",
      "+-----------+--------+------+----+\n",
      "|          1|    1001|   100|john|\n",
      "|          1|    1004|   200|john|\n",
      "|          3|    1003|   300|tony|\n",
      "|          2|    1002|   200|mike|\n",
      "|          4|    null|  null|kent|\n",
      "+-----------+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"right_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "crazy-scout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+\n",
      "|customer_id|order_id|amount|name|\n",
      "+-----------+--------+------+----+\n",
      "|          1|    1001|   100|john|\n",
      "|          1|    1004|   200|john|\n",
      "|          3|    1003|   300|tony|\n",
      "|          2|    1002|   200|mike|\n",
      "|          4|    null|  null|kent|\n",
      "+-----------+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"rightouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-collection",
   "metadata": {},
   "source": [
    "FULL JOIN - fetches data if present in either of the two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "marked-primary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>tony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  order_id  amount  name\n",
       "0            6    1005.0   200.0  None\n",
       "1            1    1001.0   100.0  john\n",
       "2            1    1004.0   200.0  john\n",
       "3            3    1003.0   300.0  tony\n",
       "4            2    1002.0   200.0  mike\n",
       "5            4       NaN     NaN  kent"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"full\").toPandas()#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "strategic-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+\n",
      "|customer_id|order_id|amount|name|\n",
      "+-----------+--------+------+----+\n",
      "|          6|    1005|   200|null|\n",
      "|          1|    1001|   100|john|\n",
      "|          1|    1004|   200|john|\n",
      "|          3|    1003|   300|tony|\n",
      "|          2|    1002|   200|mike|\n",
      "|          4|    null|  null|kent|\n",
      "+-----------+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"fullouter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "industrial-distribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+\n",
      "|customer_id|order_id|amount|name|\n",
      "+-----------+--------+------+----+\n",
      "|          6|    1005|   200|null|\n",
      "|          1|    1001|   100|john|\n",
      "|          1|    1004|   200|john|\n",
      "|          3|    1003|   300|tony|\n",
      "|          2|    1002|   200|mike|\n",
      "|          4|    null|  null|kent|\n",
      "+-----------+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"full_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "piano-venice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+\n",
      "|customer_id|order_id|amount|name|\n",
      "+-----------+--------+------+----+\n",
      "|          6|    1005|   200|null|\n",
      "|          1|    1001|   100|john|\n",
      "|          1|    1004|   200|john|\n",
      "|          3|    1003|   300|tony|\n",
      "|          2|    1002|   200|mike|\n",
      "|          4|    null|  null|kent|\n",
      "+-----------+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-induction",
   "metadata": {},
   "source": [
    "cross join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hairy-pressing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.crossJoin.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "significant-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|customer_id|order_id|amount|\n",
      "+-----------+--------+------+\n",
      "|          1|    1001|   100|\n",
      "|          1|    1004|   200|\n",
      "|          3|    1003|   300|\n",
      "|          2|    1002|   200|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"semi\").show()\n",
    "#semi, leftsemi, left_semi, anti, leftanti and left_anti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "foster-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|customer_id|order_id|amount|\n",
      "+-----------+--------+------+\n",
      "|          1|    1001|   100|\n",
      "|          1|    1004|   200|\n",
      "|          3|    1003|   300|\n",
      "|          2|    1002|   200|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "buried-taxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|customer_id|order_id|amount|\n",
      "+-----------+--------+------+\n",
      "|          1|    1001|   100|\n",
      "|          1|    1004|   200|\n",
      "|          3|    1003|   300|\n",
      "|          2|    1002|   200|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "running-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|customer_id|order_id|amount|\n",
      "+-----------+--------+------+\n",
      "|          6|    1005|   200|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "focal-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|customer_id|order_id|amount|\n",
      "+-----------+--------+------+\n",
      "|          6|    1005|   200|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "close-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|customer_id|order_id|amount|\n",
      "+-----------+--------+------+\n",
      "|          6|    1005|   200|\n",
      "+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left.join(df_right,on=\"customer_id\",how=\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "elementary-cooling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>tony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>kent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1002</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1002</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>tony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1002</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>kent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1003</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1003</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1003</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>tony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1003</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "      <td>kent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>tony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>kent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1005</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1005</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>mike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1005</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>tony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1005</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>kent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_id  customer_id  amount  customer_id  name\n",
       "0       1001            1     100            1  john\n",
       "1       1001            1     100            2  mike\n",
       "2       1001            1     100            3  tony\n",
       "3       1001            1     100            4  kent\n",
       "4       1002            2     200            1  john\n",
       "5       1002            2     200            2  mike\n",
       "6       1002            2     200            3  tony\n",
       "7       1002            2     200            4  kent\n",
       "8       1003            3     300            1  john\n",
       "9       1003            3     300            2  mike\n",
       "10      1003            3     300            3  tony\n",
       "11      1003            3     300            4  kent\n",
       "12      1004            1     200            1  john\n",
       "13      1004            1     200            2  mike\n",
       "14      1004            1     200            3  tony\n",
       "15      1004            1     200            4  kent\n",
       "16      1005            6     200            1  john\n",
       "17      1005            6     200            2  mike\n",
       "18      1005            6     200            3  tony\n",
       "19      1005            6     200            4  kent"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_left.crossJoin(df_right).toPandas()#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-tooth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-trainer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-electronics",
   "metadata": {},
   "source": [
    "- Using the above optimizations with Arrow will produce the same results as when Arrow is not enabled. \n",
    "\n",
    "- Note that even with Arrow, toPandas() results in the collection of all records in the DataFrame to the driver program and should be done on a small subset of the data. \n",
    "\n",
    "- Not all Spark data types are currently supported and an error can be raised if a column has an unsupported type, see Supported SQL Types. If an error occurs during createDataFrame(), Spark will fall back to create the DataFrame without Arrow.\n",
    "\n",
    "- Supported SQL Types\n",
    "\n",
    "  - Currently, all Spark SQL data types are supported by Arrow-based conversion except MapType, ArrayType of TimestampType, and nested StructType."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-battlefield",
   "metadata": {},
   "source": [
    "<a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-bread",
   "metadata": {},
   "source": [
    "## 1b. Pandas UDFs (Vectorized UDFs)\n",
    "\n",
    "\n",
    "```{figure} img/chapter9/1b.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-dakota",
   "metadata": {},
   "source": [
    "- Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data to perform  vectorized operations. \n",
    "\n",
    "- A Pandas UDF is defined using the pandas_udf as a decorator.\n",
    "\n",
    "- Pandas UDFs used to be defined with Python type hints.\n",
    "\n",
    "- Note that the type hint should use pandas.Series in all cases but there is one variant that pandas.DataFrame should be used for its input or output type hint instead when the input or output column is of StructType. \n",
    "\n",
    "- The following example shows a Pandas UDF which takes long column, string column and struct column, and outputs a struct column. It requires the function to specify the type hints of pandas.Series and pandas.DataFrame as below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "roman-angle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city_struct: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      "\n",
      "+---+----+-----------+\n",
      "| id|name|city_struct|\n",
      "+---+----+-----------+\n",
      "|  1|tony|  [seattle]|\n",
      "+---+----+-----------+\n",
      "\n",
      "root\n",
      " |-- pandas_udf: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n",
      "+------------+\n",
      "|  pandas_udf|\n",
      "+------------+\n",
      "|[seattle, 5]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"city string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "# Create a Spark DataFrame that has three columns including a sturct column.\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"tony\", (\"seattle\",)]],\n",
    "    \"id long, name string, city_struct struct<city:string>\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df_pandas = df.select(func(\"id\", \"name\", \"city_struct\").alias(\"pandas_udf\"))\n",
    "df_pandas.printSchema()\n",
    "df_pandas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-arthritis",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "involved-begin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>city_struct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>tony</td>\n",
       "      <td>{'city': 'seattle'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pandas_udf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>{'city': 'seattle', 'col2': 5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_pandas.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "efficient-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city_struct: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city_struct: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_html(df.printSchema())   \n",
    "display_html(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "reverse-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = df._jdf.schema().treeString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "solid-internship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city_struct: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city_struct: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(var1,end ='');\n",
    "print(var1),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-methodology",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-prescription",
   "metadata": {},
   "source": [
    "## 1c. How to extract the keys from a map column?\n",
    "\n",
    "\n",
    "```{figure} img/chapter9/1c.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-newport",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.map_keys(col)</b>\n",
    "\n",
    "Returns an unordered array containing the keys of the map.\n",
    "\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "- col – name of column or expression\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-criminal",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame consisting of a map column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "irish-papua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n",
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "\n",
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))\n",
    "# 0    1\n",
    "# 1    4\n",
    "# 2    9\n",
    "# dtype: int64\n",
    "\n",
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()\n",
    "# +-------------------+\n",
    "# |multiply_func(x, x)|\n",
    "# +-------------------+\n",
    "# |                  1|\n",
    "# |                  4|\n",
    "# |                  9|\n",
    "# +-------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-maintenance",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame consisting of a column of keys </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "further-closure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     keys|\n",
      "+---------+\n",
      "|[a, b, c]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "df_keys = df2.select(map_keys(df2.data).alias(\"keys\"))\n",
    "df_keys.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-miracle",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "necessary-blair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input                      output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>{'a': 1, 'b': None, 'c': 3}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[a, b, c]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"input                     \",            \"output\")\n",
    "display_side_by_side(df2.toPandas(),df_keys.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-think",
   "metadata": {},
   "source": [
    "<a id='5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-advance",
   "metadata": {},
   "source": [
    "## 1d. Iterator of Series to Iterator of Series\n",
    "\n",
    "An iterator UDF is the same as a scalar pandas UDF except:\n",
    "\n",
    "The Python function\n",
    "\n",
    "- Takes an iterator of batches instead of a single input batch as input.\n",
    "- Returns an iterator of output batches instead of a single output batch.\n",
    "- The length of the entire output in the iterator should be the same as the length of the entire input.\n",
    "- The wrapped pandas UDF takes a single Spark column as an input.\n",
    "- You should specify the Python type hint as Iterator[pandas.Series] -> Iterator[pandas.Series].\n",
    "- This pandas UDF is useful when the UDF execution requires initializing some state, for example, loading a machine learning model file to apply inference to every input batch.\n",
    "\n",
    "The following example shows how to create a pandas UDF with iterator support.\n",
    "\n",
    "```{figure} img/chapter9/1d.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "portuguese-sterling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n",
      "+-----------+\n",
      "|plus_one(x)|\n",
      "+-----------+\n",
      "|        201|\n",
      "|        302|\n",
      "|        403|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()\n",
    "\n",
    "var_bc = spark.sparkContext.broadcast(100)\n",
    "\n",
    "def calculate_complex(var1,var2):\n",
    "    return var1+var2+var1*var2\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "@pandas_udf(\"long\")\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    var = var_bc.value\n",
    "    for x in iterator:\n",
    "        yield calculate_complex(x , var)\n",
    "\n",
    "df_out = df.select(plus_one(\"x\"))\n",
    "df_out.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sorted-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>plus_one(x)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_out.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-corps",
   "metadata": {},
   "source": [
    "<a id='6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-leader",
   "metadata": {},
   "source": [
    "## 1e. Iterator of Multiple Series to Iterator of Series\n",
    "\n",
    "- An Iterator of multiple Series to Iterator of Series UDF has similar characteristics and restrictions as Iterator of Series to Iterator of Series UDF. \n",
    "- The specified function takes an iterator of batches and outputs an iterator of batches. \n",
    "- It is also useful when the UDF execution requires initializing some state.\n",
    "\n",
    "The differences are:\n",
    "- The underlying Python function takes an iterator of a tuple of pandas Series.\n",
    "- The wrapped pandas UDF takes multiple Spark columns as an input.\n",
    "\n",
    "```{figure} img/chapter9/1e.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-marijuana",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array_sort(col)</b>\n",
    "\n",
    "sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col – name of column or expression\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-carrier",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with map column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "unusual-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>run_ml_model(weight, height)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Iterator, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "pdf = pd.DataFrame([(1,2,),(3,4,),(5,6)], columns=[\"weight\",\"height\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "\n",
    "var_bc = spark.sparkContext.broadcast(5)\n",
    "def calculate_complex_mul(var1,var2,var3):\n",
    "    return var1+var2+var3\n",
    "\n",
    "\n",
    "# # Declare the function and create the UDF\n",
    "@pandas_udf(\"long\")\n",
    "def run_ml_model(iterator: Iterator[Tuple[pd.Series,  pd.Series]]) -> Iterator[pd.Series]:\n",
    "    var = var_bc.value\n",
    "    for a, b in iterator:\n",
    "        yield calculate_complex_mul(a,b,var)\n",
    "\n",
    "df_out_mul = df.select(run_ml_model(\"weight\", \"height\"))\n",
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_out_mul.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-discovery",
   "metadata": {},
   "source": [
    "<a id='7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-potter",
   "metadata": {},
   "source": [
    "## 1f. Series to Scalar\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/chapter9/1f.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-northern",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.map_from_arrays(col1, col2)</b>\n",
    "\n",
    "Creates a new map from two arrays.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col1 – name of column containing a set of keys. All elements should not be null\n",
    "- col2 – name of column containing a set of values\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-disabled",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame  </b>\n",
    "\n",
    "<b>Output: Mean of the input columns  </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "completed-reservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>mean_udf(v)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df_sca = df.select(mean_udf(df['v']))\n",
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_sca.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "earned-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n",
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.5|\n",
      "|  1| 2.0|   1.5|\n",
      "|  2| 3.0|   6.0|\n",
      "|  2| 5.0|   6.0|\n",
      "|  2|10.0|   6.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n",
    "w = Window \\\n",
    "    .partitionBy('id') \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-detection",
   "metadata": {},
   "source": [
    "## 1g. Grouped Map\n",
    "\n",
    "\n",
    "Grouped map operations with Pandas instances are supported by DataFrame.groupby().applyInPandas() which requires a Python function that takes a pandas.DataFrame and return another pandas.DataFrame. It maps each group to each pandas.DataFrame in the Python function.\n",
    "\n",
    "This API implements the “split-apply-combine” pattern which consists of three steps:\n",
    "\n",
    "- Split the data into groups by using DataFrame.groupBy.\n",
    "- Apply a function on each group. The input and output of the function are both pandas.DataFrame. The input data - contains all the rows and columns for each group.\n",
    "- Combine the results into a new PySpark DataFrame.\n",
    "\n",
    "To use groupBy().applyInPandas(), the user needs to define the following:\n",
    "\n",
    "A Python function that defines the computation for each group.\n",
    "A StructType object or a string that defines the schema of the output PySpark DataFrame.\n",
    "The column labels of the returned pandas.DataFrame must either match the field names in the defined output schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices. See pandas.DataFrame on how to label columns when constructing a pandas.DataFrame.\n",
    "\n",
    "Note that all data for a group will be loaded into memory before the function is applied. This can lead to out of memory exceptions, especially if the group sizes are skewed. The configuration for maxRecordsPerBatch is not applied on groups and it is up to the user to ensure that the grouped data will fit into the available memory.\n",
    "\n",
    "The following example shows how to use groupby().applyInPandas() to subtract the mean from each value in the group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-maker",
   "metadata": {},
   "source": [
    "Setting Arrow Batch Size\n",
    "\n",
    "Data partitions in Spark are converted into Arrow record batches, which can temporarily lead to high memory usage in the JVM. To avoid possible out of memory exceptions, the size of the Arrow record batches can be adjusted by setting the conf “spark.sql.execution.arrow.maxRecordsPerBatch” to an integer that will determine the maximum number of rows for each batch. The default value is 10,000 records per batch. If the number of columns is large, the value should be adjusted accordingly. Using this limit, each data partition will be made into 1 or more record batches for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-eugene",
   "metadata": {},
   "source": [
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.GroupedData.applyInPandas(func, schema)</b>\n",
    "\n",
    "Maps each group of the current DataFrame using a pandas udf and returns the result as a DataFrame.\n",
    "\n",
    "The function should take a pandas.DataFrame and return another pandas.DataFrame. For each group, all columns are passed together as a pandas.DataFrame to the user-function and the returned pandas.DataFrame are combined as a DataFrame.\n",
    "\n",
    "The schema should be a StructType describing the schema of the returned pandas.DataFrame. The column labels of the returned pandas.DataFrame must either match the field names in the defined schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices. The length of the returned pandas.DataFrame can be arbitrary.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- func – a Python native function that takes a pandas.DataFrame, and outputs a pandas.DataFrame.\n",
    "- schema – the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "gentle-whale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n",
    "\n",
    "df_group_pandas  = df.groupby(\"id\").applyInPandas(subtract_mean, schema=\"id long, v double\")\n",
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_group_pandas.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-western",
   "metadata": {},
   "source": [
    "<a id='8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-sheet",
   "metadata": {},
   "source": [
    "## 1g. Map\n",
    "\n",
    "Map operations with Pandas instances are supported by DataFrame.mapInPandas() which maps an iterator of pandas.DataFrames to another iterator of pandas.DataFrames that represents the current PySpark DataFrame and returns the result as a PySpark DataFrame. \n",
    "\n",
    "The functions takes and outputs an iterator of pandas.DataFrame. \n",
    "\n",
    "It can return the output of arbitrary length in contrast to some Pandas UDFs although internally it works similarly with Series to Series Pandas UDF.\n",
    "\n",
    "The following example shows how to use mapInPandas():\n",
    "\n",
    "```{figure} img/chapter9/1g.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-clause",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.DataFrame.mapInPandas(func, schema)</b>\n",
    "\n",
    "Maps an iterator of batches in the current DataFrame using a Python native function that takes and outputs a pandas DataFrame, and returns the result as a DataFrame.\n",
    "\n",
    "The function should take an iterator of pandas.DataFrames and return another iterator of pandas.DataFrames. All columns are passed together as an iterator of pandas.DataFrames to the function and the returned iterator of pandas.DataFrames are combined as a DataFrame. Each pandas.DataFrame size can be controlled by spark.sql.execution.arrow.maxRecordsPerBatch.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- func – a Python native function that takes an iterator of pandas.DataFrames, and outputs an iterator of pandas.DataFrames.\n",
    "- schema – the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a    DDL-formatted type string.\n",
    "\n",
    "\n",
    "\n",
    "Maps an iterator of batches in the current DataFrame using a Python native function that takes and outputs a pandas DataFrame, and returns the result as a DataFrame.\n",
    "\n",
    "The function should take an iterator of pandas.DataFrames and return another iterator of pandas.DataFrames. All columns are passed together as an iterator of pandas.DataFrames to the function and the returned iterator of pandas.DataFrames are combined as a DataFrame. Each pandas.DataFrame size can be controlled by spark.sql.execution.arrow.maxRecordsPerBatch.\n",
    "\n",
    "Parameters\n",
    "func – a Python native function that takes an iterator of pandas.DataFrames, and outputs an iterator of pandas.DataFrames.\n",
    "schema – the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "substantial-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
    "\n",
    "def filter_func(iterator):\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]\n",
    "\n",
    "df_mapin = df.mapInPandas(filter_func, schema=df.schema)\n",
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_mapin.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-parent",
   "metadata": {},
   "source": [
    "<a id='8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-glass",
   "metadata": {},
   "source": [
    "## 1g. Co-grouped Map\n",
    "\n",
    "Co-grouped map operations with Pandas instances are supported by DataFrame.groupby().cogroup().applyInPandas() which allows two PySpark DataFrames to be cogrouped by a common key and then a Python function applied to each cogroup.\n",
    "\n",
    "It consists of the following steps:\n",
    "\n",
    "- Shuffle the data such that the groups of each dataframe which share a key are cogrouped together.\n",
    "- Apply a function to each cogroup. The input of the function is two pandas.DataFrame (with an optional tuple representing the key). The output of the function is a pandas.DataFrame.\n",
    "- Combine the pandas.DataFrames from all groups into a new PySpark DataFrame.\n",
    "\n",
    "To use groupBy().cogroup().applyInPandas(), the user needs to define the following:\n",
    "\n",
    "A Python function that defines the computation for each cogroup.\n",
    "\n",
    "A StructType object or a string that defines the schema of the output PySpark DataFrame.\n",
    "\n",
    "The column labels of the returned pandas.DataFrame must either match the field names in the defined output schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices. See pandas.DataFrame on how to label columns when constructing a pandas.DataFrame.\n",
    "\n",
    "Note that all data for a cogroup will be loaded into memory before the function is applied. This can lead to out of memory exceptions, especially if the group sizes are skewed. The configuration for maxRecordsPerBatch is not applied and it is up to the user to ensure that the cogrouped data will fit into the available memory.\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/chapter9/1g.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-twenty",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.PandasCogroupedOps(gd1, gd2)</b>\n",
    "\n",
    "Applies a function to each cogroup using pandas and returns the result as a DataFrame.\n",
    "\n",
    "The function should take two pandas.DataFrames and return another pandas.DataFrame. \n",
    "\n",
    "For each side of the cogroup, all columns are passed together as a pandas.DataFrame to the user-function and the returned pandas.DataFrame are combined as a DataFrame.\n",
    "\n",
    "The schema should be a StructType describing the schema of the returned pandas.DataFrame. \n",
    "The column labels of the returned pandas.DataFrame must either match the field names in the defined schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices. The length of the returned pandas.DataFrame can be arbitrary.\n",
    "\n",
    "<b>Parameters</b>\n",
    "\n",
    "- func – a Python native function that takes two pandas.DataFrames, and outputs a pandas.DataFrame, or that takes one tuple (grouping keys) and two pandas DataFrame``s, and outputs a pandas ``DataFrame.\n",
    "- schema – the return type of the func in PySpark. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "sealed-rover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Input                                 Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20000101</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000101</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000102</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000102</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20000101</td>\n",
       "      <td>1</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000101</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20000101</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000102</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000101</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000102</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
    "    (\"time\", \"id\", \"v2\"))\n",
    "\n",
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
    "\n",
    "df_out = df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
    "    asof_join, schema=\"time int, id int, v1 double, v2 string\")\n",
    "print(\"                     Input                                \",            \"Output\")\n",
    "display_side_by_side(df1.toPandas(),df2.toPandas(), df_out.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-maria",
   "metadata": {},
   "source": [
    "<a id='9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-taxation",
   "metadata": {},
   "source": [
    "<a id='9'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
