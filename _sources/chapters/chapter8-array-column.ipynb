{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "actual-tanzania",
   "metadata": {},
   "source": [
    "```{figure} ../images/banner.png\n",
    "---\n",
    "align: center\n",
    "name: banner\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-microwave",
   "metadata": {},
   "source": [
    "# Chapter 8 : Array Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-ownership",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-poker",
   "metadata": {},
   "source": [
    "- Various data operations on columns containing date strings, date and timestamps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-daughter",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "\n",
    "- [1. How to deal with Array columns?](#1)\n",
    "    - [1a. How to create a array column from multiple columns?](#2)\n",
    "    - [1b. How to remove duplicate values from an array column?](#3)\n",
    "    - [1c. How to check if a value is in an array column?](#4)\n",
    "    - [1d. How to find the list of elements in column A, but not in column B without duplicates?](#5)\n",
    "    - [1e. How to sort the column array in ascending order?](#6)\n",
    "    - [1f. How to create an array from a  column value  repeated  many times?](#7)\n",
    "    - [1g. How to remove all elements equal to an element from the given array in a column?](#8)\n",
    "    - [1h. How to locate the position of first occurrence of the given value in the given array in a column?](#9)\n",
    "    - [1i. How to find the minimum value of an array in a column?](#10)\n",
    "    - [1j. How to find the maximum value of an array in a column?](#11)\n",
    "    - [1k. How to convert a column of nested arrays into a map column?](#12)\n",
    "    - [1l. How to sort an array in a column in ascending or descending order?](#13)\n",
    "    - [1m. How to slice an array in a column?](#14)\n",
    "    - [1n. How to shuffle a column containing an array?](#15)\n",
    "    - [1o. How to create a  array column  containing elements with sequence(start, stop, step)?](#16)\n",
    "    - [1p. How to reverse the order(not reverse sort) of an array in a column ?](#17)\n",
    "    - [1q. How to combine two array columns into a map column?](#18)\n",
    "    - [1r. How to convert unix timestamp to a string timestamp?](#19)\n",
    "    - [1s. How to find overlap between 2 array columns?](#20)\n",
    "    - [1t. How to flatten a column containing nested arrays?](#21)\n",
    "    - [1u. How to concatenate the elements of an array in a column?](#22)\n",
    "    - [1v. How to zip 2 array columns ?](#23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deluxe-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "from IPython.display import display_html\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html(index=False)\n",
    "        html_str+= \"\\xa0\\xa0\\xa0\"*10\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "space = \"\\xa0\" * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continental-cowboy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      require([], function() {\n",
       "      })\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "\tif (!js_urls.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.3/dist/panel.min.js\"];\n",
       "  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/dataframe.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      require([], function() {\n      })\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n\tif (!js_urls.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.3/dist/panel.min.js\"];\n  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.3/dist/css/dataframe.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      inject_raw_css(\"\\ndiv.special_table + table, th, td {\\n  border: 3px solid orange;\\n}\\n\");\n    },\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import panel as pn\n",
    "\n",
    "css = \"\"\"\n",
    "div.special_table + table, th, td {\n",
    "  border: 3px solid orange;\n",
    "}\n",
    "\"\"\"\n",
    "pn.extension(raw_css=[css])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-protein",
   "metadata": {},
   "source": [
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-scheme",
   "metadata": {},
   "source": [
    "##  Chapter Outline - Gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-douglas",
   "metadata": {},
   "source": [
    "<div class=\"special_table\"></div>\n",
    "\n",
    "[![alt](img/chapter8/1.png)](#15)\n",
    "\n",
    "click on  | any image\n",
    "---: |:--- \n",
    "[![alt](img/chapter8/2.png)](#2)| [![alt](img/chapter8/3.png)](#3)\n",
    "[![alt](img/chapter8/4.png)](#4)| [![alt](img/chapter8/5.png)](#5)\n",
    "[![alt](img/chapter8/6.png)](#6)| [![alt](img/chapter8/7.png)](#7)\n",
    "[![alt](img/chapter8/8.png)](#8)| [![alt](img/chapter8/9.png)](#9)\n",
    "[![alt](img/chapter8/10.png)](#10)| [![alt](img/chapter8/11.png)](#11)\n",
    "[![alt](img/chapter8/12.png)](#12)| [![alt](img/chapter8/13.png)](#13)\n",
    "[![alt](img/chapter8/14.png)](#14)| [![alt](img/chapter8/15.png)](#15)\n",
    "[![alt](img/chapter8/16.png)](#16)| [![alt](img/chapter8/17.png)](#17)\n",
    "[![alt](img/chapter8/18.png)](#18)| [![alt](img/chapter8/19.png)](#19)\n",
    "[![alt](img/chapter8/20.png)](#20)| [![alt](img/chapter8/21.png)](#21)\n",
    "[![alt](img/chapter8/22.png)](#22)| "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-refrigerator",
   "metadata": {},
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-license",
   "metadata": {},
   "source": [
    "## 1a. How to create a array column from multiple columns?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/1.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-verification",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array(*cols)</b>\n",
    "\n",
    "Creates a new array column.\n",
    "\n",
    "<b>Parameters</b>\n",
    "\n",
    "cols – list of column names (string) or list of Column expressions that have the same data type.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-client",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with multiple columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regulated-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+------+------+----------+\n",
      "|name|     city|age|smoker|height| birthdate|\n",
      "+----+---------+---+------+------+----------+\n",
      "|John|  Seattle| 60|  true|   1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30| false|   1.8|1990-01-01|\n",
      "|Mike| New York| 40|  true|  1.65|1980-01-01|\n",
      "+----+---------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mul = spark.createDataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')],['name', 'city', 'age', 'smoker','height', 'birthdate'])\n",
    "df_mul.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-training",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naked-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        array_column|\n",
      "+--------------------+\n",
      "|  [60, 1.7, Seattle]|\n",
      "|[30, 1.8, Cupertino]|\n",
      "|[40, 1.65, New York]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "df_array = df_mul.select(array(df_mul.age,df_mul.height,df_mul.city).alias(\"array_column\"))\n",
    "df_array.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-combining",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifty-military",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>smoker</th>\n",
       "      <th>height</th>\n",
       "      <th>birthdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>John</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1960-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Tony</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1990-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>New York</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1980-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>array_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[60, 1.7, Seattle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[30, 1.8, Cupertino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[40, 1.65, New York]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_mul.toPandas(),df_array.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-campbell",
   "metadata": {},
   "source": [
    "<a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-overall",
   "metadata": {},
   "source": [
    "## 1b. How to remove duplicate values from an array column?\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/2.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-family",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>ppyspark.sql.functions.array_distinct(col)</b>\n",
    "\n",
    "removes duplicate values from the array\n",
    "\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "col – name of column or expression\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-function",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a array column with duplicates</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fitting-fiber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           data|\n",
      "+---------------+\n",
      "|[1, 2, 3, 2, 4]|\n",
      "|[4, 5, 5, 4, 6]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array = spark.createDataFrame([([1, 2, 3, 2, 4],), ([4, 5, 5, 4, 6],)], ['data'])\n",
    "df_array.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-pathology",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a array column with no duplicates</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cosmetic-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_no_dup|\n",
      "+------------+\n",
      "|[1, 2, 3, 4]|\n",
      "|   [4, 5, 6]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_distinct\n",
    "df_array_no = df_array.select(array_distinct(df_array.data).alias(\"array_no_dup\"))\n",
    "df_array_no.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-interview",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conscious-international",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 2, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 5, 4, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>array_no_dup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_array.toPandas(),df_array_no.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-costs",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-digit",
   "metadata": {},
   "source": [
    "## 1c. How to check if a value is in an array column?\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/3.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-confirmation",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array_contains(col, value)</b>\n",
    "\n",
    "returns null if the array is null, true if the array contains the given value, and false otherwise.\n",
    "\n",
    "\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "- col – name of column containing array\n",
    "- value – value or column to check for in array\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-bradley",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a array column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "driven-royal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     data|\n",
      "+---------+\n",
      "|[1, 2, 3]|\n",
      "|       []|\n",
      "|      [,]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([([1, 2, 3],), ([],),([None, None],)], ['data'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-diagram",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a column to indicate if a value exists </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "forced-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|if_1_exists|\n",
      "+-----------+\n",
      "|       true|\n",
      "|      false|\n",
      "|       null|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df2 = df1.select(array_contains(df1.data, 1).alias(\"if_1_exists\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-project",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adjusted-alaska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input                      output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[None, None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>if_1_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"input                     \",            \"output\")\n",
    "display_side_by_side(df1.toPandas(),df2.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-following",
   "metadata": {},
   "source": [
    "<a id='5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-program",
   "metadata": {},
   "source": [
    "## 1d. How to find the list of elements in column A, but not in column B without duplicates?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/4.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-reader",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.array_except(col1, col2)</b>\n",
    "\n",
    "returns an array of the elements in col1 but not in col2, without duplicates.\n",
    " \n",
    "\n",
    "<b>Parameters</b>\n",
    "- col1 – name of column containing array\n",
    "- col2 – name of column containing array\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-computer",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with 2 array columns </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dense-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n",
      "|              A|               B|\n",
      "+---------------+----------------+\n",
      "|[1, 2, 3, 4, 5]|[6, 7, 8, 9, 10]|\n",
      "|[4, 5, 5, 4, 6]| [6, 2, 3, 2, 4]|\n",
      "+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.createDataFrame([([1, 2, 3, 4, 5],[6, 7, 8, 9, 10]), ([4, 5, 5, 4, 6],[6, 2, 3, 2, 4])], ['A', 'B'])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-colon",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a result array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "offshore-projector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|  in_A_not_in_B|\n",
      "+---------------+\n",
      "|[1, 2, 3, 4, 5]|\n",
      "|            [5]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_except\n",
    "df4 = df3.select(array_except(df3.A, df3.B).alias(\"in_A_not_in_B\"))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-vision",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adopted-barrel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>[6, 7, 8, 9, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 5, 4, 6]</td>\n",
       "      <td>[6, 2, 3, 2, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>in_A_not_in_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df3.toPandas(),df4.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-observation",
   "metadata": {},
   "source": [
    "<a id='6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-sucking",
   "metadata": {},
   "source": [
    "## 1e.How to sort the column array in ascending order?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/5.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-jordan",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array_sort(col)</b>\n",
    "\n",
    "sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col – name of column or expression\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-three",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pleasant-underwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               data|\n",
      "+-------------------+\n",
      "|[2, 1,, 3, 8, 3, 5]|\n",
      "|                [1]|\n",
      "|                 []|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr = spark.createDataFrame([([2, 1, None, 3, 8, 3, 5],),([1],),([],)], ['data'])\n",
    "df_arr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-hampshire",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a sorted array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "white-omega",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               sort|\n",
      "+-------------------+\n",
      "|[1, 2, 3, 3, 5, 8,]|\n",
      "|                [1]|\n",
      "|                 []|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_sort\n",
    "df_sort =df_arr.select(array_sort(df_arr.data).alias('sort'))\n",
    "df_sort.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-viewer",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "right-military",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[2, 1, None, 3, 8, 3, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 3, 5, 8, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_arr.toPandas(),df_sort.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-walter",
   "metadata": {},
   "source": [
    "<a id='7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-theta",
   "metadata": {},
   "source": [
    "## 1f. How to create an array from a  column value  repeated  many times?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/6.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-alignment",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array_repeat(col, count)</b>\n",
    "\n",
    "\n",
    "Collection function: creates an array containing a column repeated count times.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-maldives",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "yellow-initial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|data|\n",
      "+----+\n",
      "|   5|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_val = spark.createDataFrame([(5,)], ['data'])\n",
    "df_val.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-london",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a column of array of repeated values</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "worst-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   repeat|\n",
      "+---------+\n",
      "|[5, 5, 5]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_repeat\n",
    "df_repeat = df_val.select(array_repeat(df_val.data, 3).alias('repeat'))\n",
    "df_repeat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-groove",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "challenging-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>repeat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[5, 5, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_val.toPandas(),df_repeat.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-behavior",
   "metadata": {},
   "source": [
    "<a id='8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-offer",
   "metadata": {},
   "source": [
    "## 1g. How to remove all elements equal to an element from the given array in a column?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/7.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-preservation",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array_remove(col, element)</b>\n",
    "\n",
    "Remove all elements that equal to element from the given array.\n",
    "\n",
    "<b>Parameters</b>\n",
    "\n",
    "- col – name of column containing array\n",
    "- element – element to be removed from the array\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-color",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "quick-virus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             data|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 8, 4]|\n",
      "|[4, 5, 32, 32, 6]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr2 = spark.createDataFrame([([1, 2, 3, 8, 4],), ([4, 5, 32, 32, 6],)], ['data'])\n",
    "df_arr2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-consistency",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with an array column with an element removed</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "present-guard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|array_remove_4|\n",
      "+--------------+\n",
      "|  [1, 2, 3, 8]|\n",
      "|[5, 32, 32, 6]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_remove\n",
    "df_arr3 = df_arr2.select(array_remove(df_arr2.data, 4).alias(\"array_remove_4\"))\n",
    "df_arr3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-consumption",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "confirmed-dividend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>array_remove_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_arr2.toPandas(),df_arr3.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-potential",
   "metadata": {},
   "source": [
    "<a id='9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-broadcasting",
   "metadata": {},
   "source": [
    "<a id='9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-while",
   "metadata": {},
   "source": [
    "## 1h . How to locate the position of first occurrence of the given value in the given array in a column?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/8.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-might",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array_position(col, value)</b>\n",
    "\n",
    "Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-ferry",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "affected-machinery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             data|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 8, 4]|\n",
      "|[4, 5, 32, 32, 6]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pos1 = spark.createDataFrame([([1, 2, 3, 8, 4],), ([4, 5, 32, 32, 6],)], ['data'])\n",
    "df_pos1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-panel",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with column giving the position of the element </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "boxed-tanzania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|array_position_4|\n",
      "+----------------+\n",
      "|               5|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_position\n",
    "df_pos2 = df_pos1.select(array_position(df_pos1.data, 4).alias(\"array_position_4\"))\n",
    "df_pos2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-agency",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "crude-hanging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>array_position_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_pos1.toPandas(),df_pos2.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-investigator",
   "metadata": {},
   "source": [
    "<a id='10'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-quarter",
   "metadata": {},
   "source": [
    "## 1i. How to find the minimum value of an array in a column?\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/9.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-merchandise",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array_min(col)</b>\n",
    "\n",
    "returns the minimum value of the array.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col – name of column or expression\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-locator",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "knowing-packing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             data|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 8, 4]|\n",
      "|[4, 5, 32, 32, 6]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr = spark.createDataFrame([([1, 2, 3, 8, 4],), ([4, 5, 32, 32, 6],)], ['data'])\n",
    "df_arr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-nirvana",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "color-julian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|array_min|\n",
      "+---------+\n",
      "|        1|\n",
      "|        4|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_min\n",
    "df_min = df_arr.select(array_min(df_arr.data).alias(\"array_min\"))\n",
    "df_min.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-hanging",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "special-replication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>array_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_arr.toPandas(),df_min.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-worker",
   "metadata": {},
   "source": [
    "<a id='11'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-gates",
   "metadata": {},
   "source": [
    "## 1j. How to find the maximum value of an array in a column?\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/10.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-williams",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.array_max(col)</b>\n",
    "\n",
    "returns the maximum value of the array.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col – name of column or expression\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-integrity",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "norwegian-harmony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             data|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 8, 4]|\n",
      "|[4, 5, 32, 32, 6]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3, 8, 4],), ([4, 5, 32, 32, 6],)], ['data'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-anger",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "lyric-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|array_max|\n",
      "+---------+\n",
      "|        8|\n",
      "|       32|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_max\n",
    "df_max = df.select(array_max(df.data).alias(\"array_max\"))\n",
    "df_max.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-separate",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nutritional-attention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input                      output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>array_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"input                     \",            \"output\")\n",
    "display_side_by_side(df.toPandas(),df_max.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-graphic",
   "metadata": {},
   "source": [
    "<a id='12'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-quantity",
   "metadata": {},
   "source": [
    "## 1k. How to convert a column of nested arrays into a map column?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/11.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-zimbabwe",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.map_from_arrays(col1, col2)</b>\n",
    "\n",
    "Creates a new map from two arrays.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col1 – name of column containing a set of keys. All elements should not be null\n",
    "- col2 – name of column containing a set of values\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-motivation",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a map column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "portable-worse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            data|\n",
      "+----------------+\n",
      "|[[1, a], [2, b]]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-flash",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a date column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "crude-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             map|\n",
      "+----------------+\n",
      "|[1 -> a, 2 -> b]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_from_entries\n",
    "df_map = df.select(map_from_entries(\"data\").alias(\"map\"))\n",
    "df_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-portugal",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "constitutional-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[(1, a), (2, b)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>{1: 'a', 2: 'b'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_map.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-identifier",
   "metadata": {},
   "source": [
    "<a id='13'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-theater",
   "metadata": {},
   "source": [
    "## 1l. How to sort an array in a column in ascending or descending order?\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/12.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-pioneer",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.functions.sort_array(col, asc=True)</b>\n",
    "\n",
    "sorts the input array in ascending or descending order according to the natural ordering of the array elements. Null elements will be placed at the beginning of the returned array in ascending order or at the end of the returned array in descending order.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col – name of column or expression\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-timing",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "corporate-advice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             data|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 8, 4]|\n",
      "|[4, 5, 32, 32, 6]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3, 8, 4],), ([4, 5, 32, 32, 6],)], ['data'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-building",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a sorted array column  </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "affiliated-compilation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              asc|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 4, 8]|\n",
      "|[4, 5, 6, 32, 32]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sort_array\n",
    "df_asc = df.select(sort_array(df.data, asc=True).alias('asc'))\n",
    "df_asc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "educated-worse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             desc|\n",
      "+-----------------+\n",
      "|  [8, 4, 3, 2, 1]|\n",
      "|[32, 32, 6, 5, 4]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sort_array\n",
    "df_desc = df.select(sort_array(df.data, asc=False).alias('desc'))\n",
    "df_desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-bristol",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "derived-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>asc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 4, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 6, 32, 32]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[8, 4, 3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[32, 32, 6, 5, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_asc.toPandas(), df_desc.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-worth",
   "metadata": {},
   "source": [
    "<a id='15'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-pound",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "medical-bulgaria",
   "metadata": {},
   "source": [
    "<a id='14'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-peripheral",
   "metadata": {},
   "source": [
    "## 1m. How to slice an array in a column?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/13.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-giving",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.map_from_arrays(col1, col2)</b>\n",
    "\n",
    "Creates a new map from two arrays.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col1 – name of column containing a set of keys. All elements should not be null\n",
    "- col2 – name of column containing a set of values\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-variance",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "following-scheme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             data|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 8, 4]|\n",
      "|[4, 5, 32, 32, 6]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3, 8, 4],), ([4, 5, 32, 32, 6],)], ['data'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-oxygen",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with an array column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "disciplinary-vertical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      slice|\n",
      "+-----------+\n",
      "|  [2, 3, 8]|\n",
      "|[5, 32, 32]|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import slice\n",
    "df.select(slice(df.data, 2, 3).alias('slice')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-metro",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "toxic-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>{1: 'a', 2: 'b'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_map.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-magic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-buffer",
   "metadata": {},
   "source": [
    "<a id='15'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-stage",
   "metadata": {},
   "source": [
    "## 1n. How to shuffle a column containing an array?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/14.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-relations",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.shuffle(col)</b>\n",
    "\n",
    "Generates a random permutation of the given array.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-cocktail",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dated-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             data|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 8, 4]|\n",
      "|[4, 5, 32, 32, 6]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2, 3, 8, 4],), ([4, 5, 32, 32, 6],)], ['data'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-sleeve",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with shuffled array column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mental-session",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          shuffle|\n",
      "+-----------------+\n",
      "|  [2, 4, 1, 3, 8]|\n",
      "|[6, 5, 32, 32, 4]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import shuffle\n",
    "df_shu = df.select(shuffle(df.data).alias('shuffle'))\n",
    "df_shu.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-springfield",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "saved-receiver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>shuffle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[2, 4, 1, 3, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[6, 5, 32, 32, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_shu.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-prince",
   "metadata": {},
   "source": [
    "<a id='16'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-finland",
   "metadata": {},
   "source": [
    "## 1o. How to create a  array column  containing elements with sequence(start, stop, step)?\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/15.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-repository",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.sequence(start, stop, step=None)</b>\n",
    "\n",
    "Generate a sequence of integers from start to stop, incrementing by step. If step is not set, incrementing by 1 if start is less than or equal to stop, otherwise -1.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-listening",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "south-brain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "| -2|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(-2, 2)], ('A', 'B'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-sewing",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with an array sequence</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sealed-convenience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              seq|\n",
      "+-----------------+\n",
      "|[-2, -1, 0, 1, 2]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sequence\n",
    "df_seq = df.select(sequence('A', 'B').alias('seq'))\n",
    "df_seq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-entrance",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "forty-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[-2, -1, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df.toPandas(),df_seq.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-halloween",
   "metadata": {},
   "source": [
    "<a id='17'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-burlington",
   "metadata": {},
   "source": [
    "## 1p. How to reverse the order(not reverse sort) of an array in a column ?\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/16.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-lancaster",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.map_from_arrays(col1, col2)</b>\n",
    "\n",
    "Creates a new map from two arrays.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col1 – name of column containing a set of keys. All elements should not be null\n",
    "- col2 – name of column containing a set of values\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-munich",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "legal-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             data|\n",
      "+-----------------+\n",
      "|  [1, 2, 3, 8, 4]|\n",
      "|[4, 5, 32, 32, 6]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr = spark.createDataFrame([([1, 2, 3, 8, 4],), ([4, 5, 32, 32, 6],)], ['data'])\n",
    "df_arr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-pierce",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a reverse ordered array column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "above-german",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|reverse_order    |\n",
      "+-----------------+\n",
      "|[4, 8, 3, 2, 1]  |\n",
      "|[6, 32, 32, 5, 4]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import reverse\n",
    "df_rev = df_arr.select(reverse(df_arr.data).alias('reverse_order'))\n",
    "df_rev.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-convention",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "linear-festival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>reverse_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[4, 8, 3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[6, 32, 32, 5, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_arr.toPandas(),df_rev.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-paris",
   "metadata": {},
   "source": [
    "<a id='18'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-vitamin",
   "metadata": {},
   "source": [
    "## 1q. How to combine two array columns into a map column?\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/17.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-difficulty",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.map_from_arrays(col1, col2)</b>\n",
    "\n",
    "Creates a new map from two arrays.\n",
    "\n",
    "<b>Parameters</b>\n",
    "- col1 – name of column containing a set of keys. All elements should not be null\n",
    "- col2 – name of column containing a set of values\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-diana",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with 2 array columns </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "precious-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n",
      "|              A|               B|\n",
      "+---------------+----------------+\n",
      "|[1, 2, 3, 4, 5]|[6, 7, 8, 9, 10]|\n",
      "|[4, 5, 6, 7, 8]| [6, 2, 3, 9, 4]|\n",
      "+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arrm = spark.createDataFrame([([1, 2, 3, 4, 5],[6, 7, 8, 9, 10]), ([4, 5, 6, 7, 8],[6, 2, 3, 9, 4])], ['A','B'])\n",
    "df_arrm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-favorite",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a map column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "veterinary-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|map                                      |\n",
      "+-----------------------------------------+\n",
      "|[1 -> 6, 2 -> 7, 3 -> 8, 4 -> 9, 5 -> 10]|\n",
      "|[4 -> 6, 5 -> 2, 6 -> 3, 7 -> 9, 8 -> 4] |\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_from_arrays\n",
    "df_map = df_arrm.select(map_from_arrays(df_arrm.A, df_arrm.B).alias('map'))\n",
    "df_map.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-asset",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "moderate-junction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>[6, 7, 8, 9, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 6, 7, 8]</td>\n",
       "      <td>[6, 2, 3, 9, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>{1: 6, 2: 7, 3: 8, 4: 9, 5: 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>{8: 4, 4: 6, 5: 2, 6: 3, 7: 9}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_arrm.toPandas(),df_map.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-garden",
   "metadata": {},
   "source": [
    "<a id='19'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-louisiana",
   "metadata": {},
   "source": [
    "## 1r. How to concatenate the elements of an array in a column?\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/18.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-illness",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.concat(*cols)</b>\n",
    "\n",
    "Concatenates multiple input columns together into a single column. The function works with strings, binary and compatible array columns.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-seminar",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a map column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "german-scratch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n",
      "|              A|               B|\n",
      "+---------------+----------------+\n",
      "|[1, 2, 3, 4, 5]|[6, 7, 8, 9, 10]|\n",
      "|[4, 5, 6, 7, 8]| [6, 2, 3, 9, 4]|\n",
      "+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr1 = spark.createDataFrame([([1, 2, 3, 4, 5],[6, 7, 8, 9, 10]), ([4, 5, 6, 7, 8],[6, 2, 3, 9, 4])], ['A','B'])\n",
    "df_arr1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-albany",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a date column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "monetary-bernard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|concatenate                    |\n",
      "+-------------------------------+\n",
      "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]|\n",
      "|[4, 5, 6, 7, 8, 6, 2, 3, 9, 4] |\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "df_con = df_arr1.select(concat(df_arr1.A, df_arr1.B).alias(\"concatenate\"))\n",
    "df_con.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-comparison",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "supported-mixer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>[6, 7, 8, 9, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 6, 7, 8]</td>\n",
       "      <td>[6, 2, 3, 9, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>concatenate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 6, 7, 8, 6, 2, 3, 9, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_arr1.toPandas(),df_con.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-maria",
   "metadata": {},
   "source": [
    "<a id='20'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-nudist",
   "metadata": {},
   "source": [
    "## 1s. How to find overlap between 2 array columns?\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/chapter8/19.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-dealing",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.arrays_overlap(a1, a2)</b>\n",
    "\n",
    "Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-ecuador",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with array columns </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dominican-adrian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|     A|     B|\n",
      "+------+------+\n",
      "|[a, b]|[b, c]|\n",
      "|   [a]|[b, c]|\n",
      "|  [a,]|  [b,]|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_over = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"],), ([\"a\"], [\"b\", \"c\"],),([\"a\", None], [\"b\", None],) ], ['A', 'B'])\n",
    "df_over.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-tournament",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "collective-johns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|overlap|\n",
      "+-------+\n",
      "|   true|\n",
      "|  false|\n",
      "|   null|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import arrays_overlap\n",
    "df_overlap = df_over.select(arrays_overlap(df_over.A, df_over.B).alias(\"overlap\"))\n",
    "df_overlap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-julian",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "italian-romance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[a, b]</td>\n",
       "      <td>[b, c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[a]</td>\n",
       "      <td>[b, c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[a, None]</td>\n",
       "      <td>[b, None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_over.toPandas(),df_overlap.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-superior",
   "metadata": {},
   "source": [
    "<a id='21'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-crawford",
   "metadata": {},
   "source": [
    "## 1t. How to flatten a column containing nested arrays?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/20.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-dressing",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.flatten(col)</b>\n",
    "\n",
    "creates a single array from an array of arrays. If a structure of nested arrays is deeper than two levels, only one level of nesting is removed.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-crash",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with nested array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "automotive-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|data                         |\n",
      "+-----------------------------+\n",
      "|[[1, 2, 3, 8, 4], [6, 8, 10]]|\n",
      "|[[4, 5, 32, 32, 6]]          |\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.createDataFrame([([[1, 2, 3, 8, 4],[6,8, 10]],), ([[4, 5, 32, 32, 6]],)], ['data'])\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-stack",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a flattended array column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "computational-judge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|flatten                  |\n",
      "+-------------------------+\n",
      "|[1, 2, 3, 8, 4, 6, 8, 10]|\n",
      "|[4, 5, 32, 32, 6]        |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import flatten\n",
    "df_flat = df4.select(flatten(df4.data).alias('flatten'))\n",
    "df_flat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-chuck",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "intensive-classroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[[1, 2, 3, 8, 4], [6, 8, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[[4, 5, 32, 32, 6]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>flatten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 8, 4, 6, 8, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, 32, 32, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df4.toPandas(),df_flat.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-apparatus",
   "metadata": {},
   "source": [
    "<a id='22'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-compiler",
   "metadata": {},
   "source": [
    "## 1u. How to concatenate the elements of an array in a column?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/21.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-dining",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.array_join(col, delimiter, null_replacement=None)</b>\n",
    "\n",
    "Concatenates the elements of column using the delimiter. Null values are replaced with null_replacement if set, otherwise they are ignored.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-korea",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "united-giving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|              A|\n",
      "+---------------+\n",
      "|[1, 2, 3, 4, 5]|\n",
      "|  [4, 5,, 4, 6]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a1 = spark.createDataFrame([([1, 2, 3, 4, 5],), ([4, 5, None, 4, 6],)], ['A'])\n",
    "df_a1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-warehouse",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a concatenated array element column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "technological-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|array_join|\n",
      "+----------+\n",
      "| 1,2,3,4,5|\n",
      "|   4,5,4,6|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_join\n",
    "df_j1 = df_a1.select(array_join(df_a1.A,',').alias(\"array_join\"))\n",
    "df_j1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "behavioral-enterprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|array_join|\n",
      "+----------+\n",
      "| 1,2,3,4,5|\n",
      "|4,5,NA,4,6|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_j2 = df_a1.select(array_join(df_a1.A,',', null_replacement=\"NA\").alias(\"array_join\"))\n",
    "df_j2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-venezuela",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "molecular-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[4, 5, None, 4, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>array_join</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1,2,3,4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4,5,4,6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>array_join</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1,2,3,4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4,5,NA,4,6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(df_a1.toPandas(),df_j1.toPandas(), df_j2.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-marketing",
   "metadata": {},
   "source": [
    "<a id='23'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-pharmaceutical",
   "metadata": {},
   "source": [
    "## 1v. How to zip 2 array columns ?\n",
    "\n",
    "Explanation here.\n",
    "\n",
    "```{figure} img/chapter8/22.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-canvas",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.arrays_zip(*cols)</b>\n",
    "\n",
    "\n",
    "Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays.\n",
    "\n",
    "Parameters\n",
    "cols – columns of arrays to be merged.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-perfume",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with an array column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "gentle-merchant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|        A|        B|\n",
      "+---------+---------+\n",
      "|[1, 2, 3]|[4, 5, 6]|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfz = spark.createDataFrame([(([1, 2, 3], [4, 5, 6]))], ['A', 'B'])\n",
    "dfz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-jungle",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a zipped array column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "prostate-vacuum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|zipped                  |\n",
      "+------------------------+\n",
      "|[[1, 4], [2, 5], [3, 6]]|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import arrays_zip\n",
    "df_zip = dfz.select(arrays_zip(dfz.A, dfz.B).alias('zipped'))\n",
    "df_zip.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-mechanics",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "devoted-construction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                      Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[4, 5, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>zipped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[(1, 4), (2, 5), (3, 6)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                     \",            \"Output\")\n",
    "display_side_by_side(dfz.toPandas(),df_zip.toPandas())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
