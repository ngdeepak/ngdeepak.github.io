{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unlikely-bhutan",
   "metadata": {},
   "source": [
    "```{figure} ../images/banner.png\n",
    "---\n",
    "align: center\n",
    "name: banner\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-radar",
   "metadata": {},
   "source": [
    "# Chapter 4 : Data Types & Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-barrel",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-muscle",
   "metadata": {},
   "source": [
    "- Various Data types in Spark. \n",
    "- Use of Schema. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-nashville",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "\n",
    "- [1. Various data types ](#1)\n",
    "    - [1a. How to get the data type of a column/data frame?](#2)\n",
    "    - [1b. How to change the data type of a column?](#3)\n",
    "- [2. Various Schema operations](#4)\n",
    "    - [2a. How to get the schema of a data frame?](#5)\n",
    "    - [2b. How to define a schema?](#6)\n",
    "    - [2c. How to use the schema?](#7)\n",
    "    - [2d. How to save the schema?](#8)\n",
    "    - [2e. How to load the saved schema?](#9)\n",
    "    - [2f. How to get the names of all fields in the schema?](#10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "accepting-boston",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# import panel as pn\n",
    "# css = \"\"\"\n",
    "# div.special_table + table, th, td {\n",
    "#   border: 3px solid orange;\n",
    "# }\n",
    "# \"\"\"\n",
    "# pn.extension(raw_css=[css])\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "from IPython.display import display_html\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "def display_side_by_side(*args,space):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html(index=False)\n",
    "        html_str+= \"\\xa0\\xa0\\xa0\"*space\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "space = \"\\xa0\" * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-helena",
   "metadata": {},
   "source": [
    "### Common data types in spark\n",
    "\n",
    "\n",
    "<style> \n",
    "table td, table th, table tr {text-align:left !important;}\n",
    "</style>\n",
    "\n",
    "<b>Numeric types</b>\n",
    "\n",
    "| Name          | Description  | Example |                               \n",
    "| :-------------------- | :--------- | :------------- | \n",
    "| IntegerType              | Represents 4-byte signed integer numbers.        |64   |                      \n",
    "| LongType |Represents 8-byte signed integer numbers   | 1000  |                 |                                \n",
    "| FloatType              |Represents 4-byte single-precision floating point numbers.    | 1000.45 |               \n",
    "| DoubleType              |Represents 8-byte double-precision floating point numbers.    | 10000000.45 |    \n",
    "                        \n",
    "                        \n",
    "<b>String type</b>\n",
    "\n",
    "| Name          | Description  | Example |                               \n",
    "| :-------------------- | :--------- | :------------- |\n",
    "| StringType              |Represents character string values.       |\"tony\"   |                      \n",
    "\n",
    "<b>Boolean type</b>\n",
    "\n",
    "| Name          | Description  | Example |                               \n",
    "| :-------------------- | :--------- | :------------- |\n",
    "| BooleanType              |Represents boolean values.        |\"true\"   | \n",
    "\n",
    "<b>Datetime type</b>\n",
    "\n",
    "| Name          | Description  | Example |                               \n",
    "| :-------------------- | :--------- | :------------- |\n",
    "| TimestampType             |Represents values comprising values of fields year, month, day, hour, minute, and second, with the session local time-zone. The timestamp value represents an absolute point in time       |2019-11-03 05:30:00 UTC-05:00   | \n",
    "| DateType             |Represents values comprising values of fields year, month and day, without a time-zone.       |2019-11-03  | \n",
    "\n",
    "\n",
    "<b>Complex types</b>\n",
    "\n",
    "| Name          | Definition  | Description | Example |                               \n",
    "| :-------------------- | :--------- | :------------- |:------------- |\n",
    "| ArrayType             |ArrayType(elementType, containsNull)       |Represents values comprising a sequence of elements with the type of elementType. containsNull is used to indicate if elements in a ArrayType value can have null values.  | [\"tony\",\"kent\", \"mike\"]   |\n",
    "| MapType              |MapType(keyType, valueType, valueContainsNull):      | Represents values comprising a set of key-value pairs. The data type of keys is described by keyType and the data type of values is described by valueType. For a MapType value, keys are not allowed to have null values. valueContainsNull is used to indicate if values of a MapType value can have null values.   |  {\"name\":\"tony\"}  |\n",
    "| StructType              |StructType(fields)       |Represents values with the structure described by a sequence of StructFields (fields).StructField(name, dataType, nullable): Represents a field in a StructType. The name of a field is indicated by name. The data type of a field is indicated by dataType. nullable is used to indicate if values of these fields can have null values.   |   {\"name\":\"tony\",\"age\":30,\"city\":\"\"seattle\"}    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-union",
   "metadata": {},
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-chamber",
   "metadata": {},
   "source": [
    "## 1a.  How to get the data type of a column/data frame?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-bookmark",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.DataFrame.dtypes</b>\n",
    "\n",
    "Returns all column names and their data types as a list.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-conservation",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ahead-galaxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+------+----------+\n",
      "|name|age|smoker|height| birthdate|\n",
      "+----+---+------+------+----------+\n",
      "|John| 60|  true|   1.7|1960-01-01|\n",
      "|Tony| 30| false|   1.8|1990-01-01|\n",
      "|Mike| 40|  true|  1.65|1980-01-01|\n",
      "+----+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mul = spark.createDataFrame([('John',  60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike',  40, True, 1.65, '1980-01-01')],['name',  'age', 'smoker','height', 'birthdate'])\n",
    "df_mul.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-auditor",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame column types</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "stunning-university",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'),\n",
       " ('age', 'bigint'),\n",
       " ('smoker', 'boolean'),\n",
       " ('height', 'double'),\n",
       " ('birthdate', 'string')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mul.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-steering",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "rolled-consent",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                                         Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>smoker</th>\n",
       "      <th>height</th>\n",
       "      <th>birthdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>John</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1960-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Tony</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1990-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1980-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">   <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[('name', 'string'), ('age', 'bigint'), ('smoker', 'boolean'), ('height', 'double'), ('birthdate', 'string')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                                        \",            \"Output\")\n",
    "display_side_by_side(df_mul.toPandas(),pd.DataFrame([str(df_mul.dtypes[0:6])],columns=[\".\"]),space=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-newport",
   "metadata": {},
   "source": [
    "<a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-findings",
   "metadata": {},
   "source": [
    "## 1b. How to change the data type of a column?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-lesbian",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.Column.cast </b>\n",
    "\n",
    "Convert the column into type dataType.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-shell",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame with a column \"age\" of integer type</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "iraqi-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+------+\n",
      "|name|age|smoker|height|\n",
      "+----+---+------+------+\n",
      "|John| 60|  true|   1.7|\n",
      "|Tony| 30| false|   1.8|\n",
      "|Mike| 40|  true|  1.65|\n",
      "+----+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mul = spark.createDataFrame([('John',  60, True, 1.7), \n",
    "('Tony', 30, False, 1.8, ), \n",
    "('Mike',  40, True, 1.65, )],['name',  'age', 'smoker','height'])\n",
    "df_mul.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "declared-offense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'),\n",
       " ('age', 'bigint'),\n",
       " ('smoker', 'boolean'),\n",
       " ('height', 'double')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mul.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-saturn",
   "metadata": {},
   "source": [
    "<b>Output :  Spark data frame with a column with a split string</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "agreed-paste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+------+\n",
      "|name|age|smoker|height|\n",
      "+----+---+------+------+\n",
      "|John| 60|  true|   1.7|\n",
      "|Tony| 30| false|   1.8|\n",
      "|Mike| 40|  true|  1.65|\n",
      "+----+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast = df_mul.select(\"name\",df_mul.age.cast(\"string\").alias('age'), \"smoker\", \"height\")\n",
    "df_cast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "seventh-anniversary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'),\n",
       " ('age', 'string'),\n",
       " ('smoker', 'boolean'),\n",
       " ('height', 'double')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cast.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-politics",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "immune-tonight",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                                                        Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>smoker</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>John</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Tony</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                                                                           <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>smoker</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>John</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Tony</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mike</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">                                                                           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types                                                       Data types\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[('name', 'string'), ('age', 'bigint'), ('smoker', 'boolean'), ('height', 'double')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">               <table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[('name', 'string'), ('age', 'string'), ('smoker', 'boolean'), ('height', 'double')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Input                                                       \",            \"Output\")\n",
    "display_side_by_side(df_mul.toPandas(),df_cast.toPandas(),space=25)\n",
    "print(\"Data types                                                      \",            \"Data types\")\n",
    "display_side_by_side(pd.DataFrame([str(df_mul.dtypes)],columns=[\".\"]),pd.DataFrame([str(df_cast.dtypes)],columns=[\".\"]),space=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-holmes",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-parking",
   "metadata": {},
   "source": [
    "## what is  Schema ?\n",
    "A schema is the description of the structure of your data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-dependence",
   "metadata": {},
   "source": [
    "<a id='5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-primary",
   "metadata": {},
   "source": [
    "## 2a. How to get the schema of a data frame?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-neutral",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "Converts a string expression to upper case.\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>pyspark.sql.DataFrame.schema</b>\n",
    "Returns the schema of this DataFrame as a pyspark.sql.types.StructType.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-seven",
   "metadata": {},
   "source": [
    "<b>Input:  Spark data frame </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "interstate-stomach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+------+------+----------+\n",
      "|name|     city|age|smoker|height| birthdate|\n",
      "+----+---------+---+------+------+----------+\n",
      "|John|  Seattle| 60|  true|   1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30| false|   1.8|1990-01-01|\n",
      "|Mike| New York| 40|  true|  1.65|1980-01-01|\n",
      "+----+---------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mul = spark.createDataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')],['name', 'city', 'age', 'smoker','height', 'birthdate'])\n",
    "df_mul.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-allen",
   "metadata": {},
   "source": [
    "<b>Output :  Schema of Spark data frame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "proprietary-fossil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(city,StringType,true),StructField(age,LongType,true),StructField(smoker,BooleanType,true),StructField(height,DoubleType,true),StructField(birthdate,StringType,true)))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mul.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-shelf",
   "metadata": {},
   "source": [
    "### How to print the schema in a tree format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "verified-dispute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- smoker: boolean (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mul.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-occurrence",
   "metadata": {},
   "source": [
    "<a id='6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-deadline",
   "metadata": {},
   "source": [
    "## 2b. How to define a schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-employee",
   "metadata": {},
   "source": [
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "\n",
    "<b>pyspark.sql.functions.slice(x, start, length)</b>\n",
    "\n",
    "Collection function: returns an array containing all the elements in x from index start (array indices start at 1, or from the end if start is negative) with the specified length.\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "- x : the array to be sliced\n",
    "\n",
    "- start : the starting index\n",
    "\n",
    "- length : the length of the slice\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-yeast",
   "metadata": {},
   "source": [
    "#### Example#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "loose-vancouver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(city,StringType,true),StructField(age,IntegerType,true),StructField(smoker,BooleanType,true),StructField(height,FloatType,true),StructField(birthdate,StringType,true)))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema1 = StructType([\n",
    "StructField(\"name\", StringType(), True), \n",
    "StructField(\"city\", StringType(), True), \n",
    "StructField(\"age\", IntegerType(), True), \n",
    "StructField(\"smoker\", BooleanType(), True), \n",
    "StructField(\"height\", FloatType(), True), \n",
    "StructField(\"birthdate\", StringType(), True), \n",
    "])\n",
    "schema1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "virtual-taylor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'city', 'age', 'smoker', 'height', 'birthdate']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema1.fieldNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-drink",
   "metadata": {},
   "source": [
    "#### Example#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "based-tribune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(name,StringType,true),StructField(weight,LongType,true),StructField(smoker,BooleanType,true),StructField(height,DoubleType,true),StructField(birthdate,StringType,true),StructField(phone_nos,MapType(StringType,LongType,true),true),StructField(favorite_colors,ArrayType(StringType,true),true),StructField(address,StructType(List(StructField(houseno,IntegerType,true),StructField(street,StringType,true),StructField(city,StringType,true),StructField(zipcode,IntegerType,true))),true)))\n"
     ]
    }
   ],
   "source": [
    "schema2 = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"weight\", LongType()),\n",
    "    StructField(\"smoker\", BooleanType()),\n",
    "    StructField(\"height\", DoubleType()),\n",
    "    StructField(\"birthdate\", StringType()),\n",
    "    StructField(\"phone_nos\", MapType(StringType(),LongType(),True),True),  \n",
    "    StructField(\"favorite_colors\", ArrayType(StringType(),True),True),  \n",
    "    StructField(\"address\", StructType([\n",
    "    StructField(\"houseno\", IntegerType(),True),\n",
    "    StructField(\"street\", StringType(),True),\n",
    "    StructField(\"city\", StringType(),True),\n",
    "    StructField(\"zipcode\", IntegerType(),True),\n",
    "    ])) \n",
    "    \n",
    "])\n",
    "print(schema2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "acknowledged-extraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'weight',\n",
       " 'smoker',\n",
       " 'height',\n",
       " 'birthdate',\n",
       " 'phone_nos',\n",
       " 'favorite_colors',\n",
       " 'address']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema2.fieldNames()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-hands",
   "metadata": {},
   "source": [
    "<a id='7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-amplifier",
   "metadata": {},
   "source": [
    "##  2c. How to use the schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "legislative-morning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      " |-- smoker: boolean (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- phone_nos: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- favorite_colors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- houseno: integer (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- zipcode: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>weight</th>\n",
       "      <th>smoker</th>\n",
       "      <th>height</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>phone_nos</th>\n",
       "      <th>favorite_colors</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john</td>\n",
       "      <td>180</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>{'office': 123456789, 'home': 223456789}</td>\n",
       "      <td>[blue, red]</td>\n",
       "      <td>(100, street1, city1, 12345)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tony</td>\n",
       "      <td>180</td>\n",
       "      <td>True</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>{'office': 223456789, 'home': 323456789}</td>\n",
       "      <td>[green, purple]</td>\n",
       "      <td>(200, street2, city2, 22345)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mike</td>\n",
       "      <td>180</td>\n",
       "      <td>True</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1980-01-01</td>\n",
       "      <td>{'office': 323456789, 'home': 423456789}</td>\n",
       "      <td>[yellow, orange]</td>\n",
       "      <td>(300, street3, city3, 32345)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  weight  smoker  height   birthdate  \\\n",
       "0  john     180    True    1.70  1960-01-01   \n",
       "1  tony     180    True    1.80  1990-01-01   \n",
       "2  mike     180    True    1.65  1980-01-01   \n",
       "\n",
       "                                  phone_nos   favorite_colors  \\\n",
       "0  {'office': 123456789, 'home': 223456789}       [blue, red]   \n",
       "1  {'office': 223456789, 'home': 323456789}   [green, purple]   \n",
       "2  {'office': 323456789, 'home': 423456789}  [yellow, orange]   \n",
       "\n",
       "                        address  \n",
       "0  (100, street1, city1, 12345)  \n",
       "1  (200, street2, city2, 22345)  \n",
       "2  (300, street3, city3, 32345)  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as func\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"weight\", LongType()),\n",
    "    StructField(\"smoker\", BooleanType()),\n",
    "    StructField(\"height\", DoubleType()),\n",
    "    StructField(\"birthdate\", StringType()),\n",
    "    StructField(\"phone_nos\", MapType(StringType(),LongType(),True),True),  \n",
    "    StructField(\"favorite_colors\", ArrayType(StringType(),True),True),  \n",
    "    StructField(\"address\", StructType([\n",
    "    StructField(\"houseno\", IntegerType(),True),\n",
    "    StructField(\"street\", StringType(),True),\n",
    "    StructField(\"city\", StringType(),True),\n",
    "    StructField(\"zipcode\", IntegerType(),True),\n",
    "    ])) \n",
    "    \n",
    "])\n",
    "\n",
    "df = spark.createDataFrame((\n",
    "    [[\"john\",180,True,1.7,'1960-01-01',{'office': 123456789, 'home': 223456789},[\"blue\",\"red\"],(100,'street1','city1',12345)],\n",
    "    [\"tony\",180,True,1.8,'1990-01-01',{'office': 223456789, 'home': 323456789},[\"green\",\"purple\"],(200,'street2','city2',22345)],\n",
    "    [\"mike\",180,True,1.65,'1980-01-01',{'office': 323456789, 'home': 423456789},[\"yellow\",\"orange\"],(300,'street3','city3',32345)]]\n",
    "),schema=schema)\n",
    "df.toPandas()#(3,False)\n",
    "df.printSchema()\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-appliance",
   "metadata": {},
   "source": [
    "<a id='8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-cloud",
   "metadata": {},
   "source": [
    "## 2d. How to save the schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "supported-transcription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- weight: long (nullable = true)\n",
      " |-- smoker: boolean (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- phone_nos: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- favorite_colors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- houseno: integer (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- zipcode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame((\n",
    "    [[\"john\",180,True,1.7,'1960-01-01',{'office': 123456789, 'home': 223456789},[\"blue\",\"red\"],(100,'street1','city1',12345)],\n",
    "    [\"tony\",180,True,1.8,'1990-01-01',{'office': 223456789, 'home': 323456789},[\"green\",\"purple\"],(200,'street2','city2',22345)],\n",
    "    [\"mike\",180,True,1.65,'1980-01-01',{'office': 323456789, 'home': 423456789},[\"yellow\",\"orange\"],(300,'street3','city3',32345)]]\n",
    "),schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "alpine-matter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(weight,LongType,true),StructField(smoker,BooleanType,true),StructField(height,DoubleType,true),StructField(birthdate,StringType,true),StructField(phone_nos,MapType(StringType,LongType,true),true),StructField(favorite_colors,ArrayType(StringType,true),true),StructField(address,StructType(List(StructField(houseno,IntegerType,true),StructField(street,StringType,true),StructField(city,StringType,true),StructField(zipcode,IntegerType,true))),true)))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ordinary-dance",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o301.saveAsObjectFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/deepak/Documents/sparkbook/chapters/data/text/schema_file already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:69)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:54)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1561)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1561)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:565)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-394ef375de97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrdd_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrdd_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsPickleFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/text/schema_file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/sparkbook/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsPickleFile\u001b[0;34m(self, path, batchSize)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsObjectFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sparkbook/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sparkbook/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/sparkbook/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o301.saveAsObjectFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/deepak/Documents/sparkbook/chapters/data/text/schema_file already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:69)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:54)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1561)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1561)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:565)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "rdd_schema = spark.sparkContext.parallelize(df.schema)\n",
    "\n",
    "rdd_schema.coalesce(1).saveAsPickleFile(\"data/text/schema_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-afternoon",
   "metadata": {},
   "source": [
    "<a id='9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-stanford",
   "metadata": {},
   "source": [
    "## 2e. How to load the saved schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_rdd = spark.sparkContext.pickleFile(\"data/text/schema_file\")\n",
    "schema = StructType(schema_rdd.collect())\n",
    "print(schema)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-encyclopedia",
   "metadata": {},
   "source": [
    "## 2f. How to get the names of all fields in the schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mul = spark.createDataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')],['name', 'city', 'age', 'smoker','height', 'birthdate'])\n",
    "df_mul.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mul.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-bobby",
   "metadata": {},
   "source": [
    "<b> Summary:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input                                    \",            \"output\")\n",
    "display_side_by_side(df_mul.toPandas(),pd.DataFrame([[df_mul.schema.fieldNames()]],columns=[\".\"]),space=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-shift",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-banner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-carol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-competition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "persistent-sunglasses",
   "metadata": {},
   "source": [
    "<a id='6'></a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
